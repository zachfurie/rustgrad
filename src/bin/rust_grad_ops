#![allow(dead_code)]

use std::{fs::File, time::{Instant}, iter::zip};
use rand::prelude::*;
use csv;
use rand_distr;
// use rayon::prelude::*;
use chrono::prelude::*;
use indicatif::{ProgressBar, ProgressStyle};


// ----------------------------------- Notes:  ----------------------------------------------

// Need to reset random seed
// Maybe use mutable arrays instead of vecs

// Not using many parallel iterators since they actually slowed down runtime. Probably because I'm already using up all the threads.

// need a function that changes drop_prob to 0 on all dropout nodes, so there is no dropout during test/val

//  ----------------------------------- Data Structures:  -----------------------------------

struct Node {
    left: Option<Box<Node>>,
	right: Option<Box<Node>>,
	op: &'static str,
	tensor: Tensor,
	grad: Tensor,
	require_grad: bool,
}

struct Tensor {
	data:  Vec<f64>, // Float64 all tensor data in a single slice 
	shape: Vec<usize>, // Int  [i] = length of dimension i
    diminc: Vec<usize>,
}

//  ----------------------------------- Auxiliary Functions:  -----------------------------------

impl Tensor {
    fn zeros(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:   vec![0.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        }
    }

    fn ones(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs
        }
    }

    fn xavier(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        let mut t = Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        };
        let std:f64;
        if shaper.len() == 4 {
            std = ((t.l2()*t.l()) as f64).sqrt();
        } else if shaper.len() == 3{
            std = (t.l() as f64).sqrt();
        } else {
            panic!("Weight initialization expects at least 3 dimensional weight (bsz, l2, l) ir 4 dimensional weight (bsz, l2, l, channels)")
        }
        let mut rng = rand::thread_rng();
        for i in 0..data_len {
            let y: f64 = rng.gen(); // generates a float between 0 and 1
            if rng.gen_bool(0.5) {
                t.data[i] = y / std; // generates a float between 0 and 1
            } else {
                t.data[i] = -1. * y / std; // generates a float between 0 and 1
            }
        };
        t
    }

    fn he(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        let mut t = Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        };
        let std:f64;
        if shaper.len() == 4 {
            std = (2. / ((t.l2()*t.l()) as f64)).sqrt(); 
        } else if shaper.len() == 3{
            std = (2. / (t.l() as f64)).sqrt(); 
        } else {
            panic!("Weight initialization expects at least 3 dimensional weight (bsz, l2, l) ir 4 dimensional weight (bsz, l2, l, channels)")
        }
        let mut rng = rand::thread_rng();
        for i in 0..data_len {
            let y: f64 = rng.sample(rand_distr::StandardNormal) ; // generates a float between 0 and 1
            t.data[i] = y * std; // generates a float between 0 and 1
        };
        t
    }

    fn get(&self, ind: &[usize]) -> f64 {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index]
    }

    fn _safe_get(&self, ind: &[usize], info: &str) -> f64 {
        let mut index = 0;
        if ind.len() > self.diminc.len() {
            panic!("ERROR: index {:?} is invalid for tensor with shape {:?}. \n Info: {:?}", ind, self.shape, info);
        } 
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        if index >= self.data.len() {
            panic!("ERROR: index {:?} is outside of tensor with shape {:?}. \n Info: {:?}", ind, self.shape, info);
        } 
        self.data[index]
    }

    fn _get_ind(&self, ind: &[usize]) -> usize {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        index
    }
    
    fn set(&mut self, ind: &[usize], elem: f64) {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index] = elem;
    }

    // Assuming data is a 2d matrix, get length of outer dimension
    fn l2(&self) -> usize {
        // self.shape[self.shape.len()-2]
        self.shape[1]
    }

    // Assuming data is a 2d matrix, get length of inner dimension
    fn l(&self) -> usize {
        // self.shape[self.shape.len()-1]
        self.shape[2]
    }

    // Get length of batch dimension
    fn bsz(&self) -> usize {
        self.shape[0]
    }

    // Get length of channels/filters dimension 
    fn chans(&self) -> usize {
        if self.shape.len() < 4 {
            panic!("Tensor of shape {:?} does not have channels", self.shape);
        }
        self.shape[3]
    }

    fn copy_tens(&self) -> Tensor {
        let new_shape = self.shape.to_vec();
        let new_data = self.data.to_vec();
        let new_incs = self.diminc.to_vec();
        Tensor {
            data:  new_data, // all tensor data in a single slice 
            shape: new_shape, // [i] = length of dimension i
            diminc: new_incs,
        }
    }

    fn reshape(&mut self, shaper: &Vec<usize>) {
        let mut data_len: usize = 1;
        let mut incs =  vec![1; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            let mut temp = 1;
            for ii in (i + 1)..shaper.len() {
                temp *= shaper[ii]
            }
            incs[i] = temp
        }
        if data_len != self.data.len() {
            panic!("Shape {:?} is not compatible with Tensor of shape {:?} and data length {}", shaper, self.shape, self.data.len());
        }
        self.shape = shaper.to_vec();
        self.diminc = incs;
    }
}

fn _add_same_size(dst: &mut Tensor, t: &Tensor ) {
    for (i,x) in t.data.iter().enumerate() {
        dst.data[i] += x
    }
}

fn _add_same_size_fast(a: &Tensor, b: &Tensor ) -> Tensor {
    let f: Vec<f64> = a.data.iter().zip(b.data.iter()).map(|(&aa, &bb)| aa + bb).collect();
    Tensor {
        data: f,
        shape: a.shape.to_vec(),
        diminc: a.diminc.to_vec(),
    }
}

fn _add_bsz_times(a: &Tensor, b: &Tensor ) -> Tensor {
    let mut f: Vec<f64> = Vec::new();
    if a.shape.len() == 3 {
        for batch in 0..a.bsz() {
            f.extend::<Vec<f64>>(zip(
                a.data[a._get_ind(&[batch,0,0])..a._get_ind(&[batch+1,0,0])].iter(),
                b.data.iter(),
            ).map(|(&aa, &bb)| {aa + bb}).collect());
        }
    } else if a.shape.len() == 4 {
        for batch in 0..a.bsz() {
            f.extend::<Vec<f64>>(zip(
                &mut a.data[a._get_ind(&[batch,0,0,0])..a._get_ind(&[batch+1,0,0,0])].iter(),
                &mut b.data.iter(),
            ).map(|(aa, bb)| {aa + bb}).collect());
        }
    }
    Tensor {
        data: f,
        shape: a.shape.to_vec(),
        diminc: a.diminc.to_vec(),
    }
}

//  ----------------------------------- BASIC OPS:  -----------------------------------

// Inputs, weights, etc
fn leaf(tens: Tensor, require_grad: bool) -> Box<Node> {
	let zero_grad = Tensor::ones(&tens.shape);
	Box::new(Node {
        left: None,
        right: None, 
        op: "leaf", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad
    })
}

// Syntactic sugar for leaf nodes that are weights
fn weight(shaper: Vec<usize>, initialization: &str) -> Box<Node> {
	let tens = match initialization {
        "xavier" => {Tensor::xavier(&shaper)}
        "he" => {Tensor::he(&shaper)}
        _ => {Tensor::xavier(&shaper)}
    };
    let zero_grad = Tensor::ones(&shaper);
	Box::new(Node {
        left: None,
        right: None, 
        op: "param", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Add two tensors. Input tensor can be batched.
fn add(bias: Box<Node>, input: Box<Node>) -> Box<Node> { //assuming right is batched input 
	let zero_grad = Tensor::ones(&input.tensor.shape);
	let data = Tensor::zeros(&input.tensor.shape);
	Box::new(Node{
        left: Some(bias), 
        right: Some(input), 
        op: "+", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Matrix Multiplication
fn matmul(weight: Box<Node>, input: Box<Node>) -> Box<Node> {
    assert!(input.tensor.l2() == 1);
	let zero_grad = Tensor::ones(&vec![input.tensor.bsz(), 1, weight.tensor.l2()]);
	let data = Tensor::zeros(&vec![input.tensor.bsz(), 1, weight.tensor.l2()]);
	Box::new(Node{
        left: Some(weight), 
        right: Some(input), 
        op: "mm",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Batch size is not included in input shape, since batch size can differ between train/test/val. Correct batch size is automatically used.
fn reshape_node(inn: Box<Node>, mut shaper: Vec<usize>) -> Box<Node> {
    let mut batched_shaper = vec![inn.tensor.bsz()];
    batched_shaper.append(&mut shaper);
	let zero_grad = Tensor::ones(&batched_shaper);
	let data = Tensor::zeros(&batched_shaper);
    let rq = inn.require_grad;
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "reshape", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: rq,
    })
}

// fn _batch_norm(a: Box<Node>)-> Box<Node> {
//     let zero_grad = Tensor::ones(&a.grad.shape);
// 	let data = Tensor::zeros(&a.tensor.shape);
//     let rq = a.require_grad;
// 	Box::new(Node{
//         left: Some(a), 
//         right: None, 
//         op: "bn",
//         tensor: data, 
//         grad: zero_grad, 
//         require_grad: rq,
//     })
// }

//  ----------------------------------- ML OPS:  -----------------------------------

// Linear layer. Returns output, weight, bias so that weight and bias can be put in params[].
fn linear(inn: Box<Node>, out_dim: usize, initialization: &str) -> Box<Node> { //  Node<'a>,  Node<'a>) {
	let l_weight = weight(vec![1,out_dim,inn.tensor.l()], initialization);
	let bias = weight(vec![1, 1, out_dim], initialization);
	let in_times_weight = matmul(l_weight,inn);
	let plus_bias = add(bias, in_times_weight);
	plus_bias
}

// Log Softmax layer
fn log_softmax(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sm", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Sigmoid layer
fn sigmoid(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sig", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Dropout 
fn dropout(inn: Box<Node>, drop_prob: f64) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
    let mut dropout_tens = Tensor::zeros(&vec![1]);
    dropout_tens.set(&[0], drop_prob);
    let dropout_node = Box::new(Node {
        left: None, 
        right: None, 
        op: "leaf", 
        tensor: dropout_tens, 
        grad: Tensor::zeros(&vec![1]), 
        require_grad: false,
    });
	Box::new(Node {
        left: Some(inn), 
        right: Some(dropout_node), 
        op: "do", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Conv2D - input is shape (batch size, rows, columns, channels), where (rows, columns) = (l2,l)
fn conv2d(input: Box<Node>, conv_size: Vec<usize>, filters: usize, initialization: &str) -> Box<Node> {
    let kernel = weight(vec![1, conv_size[0], conv_size[1], filters], initialization);
    let zero_grad = Tensor::ones(&vec![input.tensor.bsz(), 1 + input.tensor.l2() - kernel.tensor.l2(), 1 + input.tensor.l() - kernel.tensor.l(), filters]);
	let data = Tensor::zeros(&vec![input.tensor.bsz(), 1 + input.tensor.l2() - kernel.tensor.l2(), 1 + input.tensor.l() - kernel.tensor.l(), filters]);
	let ll = input.tensor.shape.to_vec();
    let mut dl = data.shape.to_vec();
    dl[0] = 1;
    let mut cv_layer = Box::new(Node{
        left: Some(input), 
        right: Some(kernel), 
        op: "c2d",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    });
    // If input has channels, sum channels before applying convolution
    if ll.len() == 4 {
        let a = cv_layer.left.unwrap();
        let sd = _sum_dim(a);
        cv_layer.left = Some(sd);
    } 
    let bias = weight(dl, initialization);
    let plus_bias = add(bias,cv_layer);
	plus_bias
}

// Helper for conv2d
fn _sum_dim(a: Box<Node>) -> Box<Node> {
    let new_shape = vec![a.tensor.bsz(), a.tensor.l2(), a.tensor.l()];
	let zero_grad = Tensor::ones(&new_shape);
	let data = Tensor::zeros(&new_shape);
    let rq = a.require_grad;
	Box::new(Node{
        left: Some(a), 
        right: None, 
        op: "sum_dim",
        tensor: data, 
        grad: zero_grad, 
        require_grad: rq,
    })
}

// ReLU
fn relu(a: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&a.grad.shape);
	let data = Tensor::zeros(&a.tensor.shape);
	Box::new(Node{
        left: Some(a), 
        right: None, 
        op: "relu",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Max Pooling 2D
fn max_pool(inn: Box<Node>, shaper: Vec<usize>) -> Box<Node> {
	let zero_grad = Tensor::ones(&vec![inn.tensor.bsz(), inn.tensor.l2() / shaper[0], inn.tensor.l() / shaper[1], inn.tensor.chans()]);
	let data = Tensor::zeros(&vec![inn.tensor.bsz(), inn.tensor.l2() / shaper[0], inn.tensor.l() / shaper[1], inn.tensor.chans()]);
    Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "mp", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// flatten input to 1 dimensional vector (batch size is maintained, this is handled by reshape_node)
fn flatten(inn: Box<Node>) -> Box<Node> {
    let shaper = inn.tensor.shape.to_vec();
    let bsz = shaper[0];
    let bigshape = shaper.iter().product::<usize>() / bsz;
    reshape_node(inn, vec![1, bigshape])
}

//  ----------------------------------- LOSS AND OPTIM:  -----------------------------------

// Negative Log Likelihood. Takes prediction tensor and target tensor as inputs, returns loss, and updates gradients from input
fn nll_loss(pred: &mut Box<Node>, target: &Tensor, _eval: bool) -> f64 { 
    // println!("pred {} | {:?}", pred.tensor.data.len(), pred.tensor.shape);
    // println!("target {} | {:?}", target.data.len(), target.shape);
	// let mut loss = 0.0;
	// for b in 0..target.bsz() {
    //     for j in 0..target.l2() {
    //         for i in 0..target.l() {
    //             let t = target.get(&[b,j,i]);
    //             loss -= t * pred.tensor.get(&[b,j,i]);
    //             pred.grad.set(&[b,j,i], t);
                
    //         }
    //     }
    // }

    assert!(pred.tensor.data.len() == target.data.len());
    let loss:f64 = zip(&pred.tensor.data, &target.data).map(|(p,t)| {
        -1.0 * p * t
    }).sum();
    if !_eval {
        pred.grad = target.copy_tens();
    }

    loss  / (target.bsz() as f64)
}

// initialize adam optimizer
struct Adam {
	// b1 float64, b2 float64
	t:        f64,// init to 0
	alpha:    f64,
	prev_m1s: Vec<Tensor>,
	prev_m2s: Vec<Tensor>,
}

impl Adam {
    //create prev_m1s and prev_m2s with same recursive search pattern (DFS)
    fn init_prevs(&mut self, w: &Box<Node>) {
        if w.op == "param" {
            let p1 = Tensor::zeros(&w.grad.shape);
            let p2 = Tensor::zeros(&w.grad.shape);
            self.prev_m1s.push(p1);
            self.prev_m2s.push(p2);
        }
        match &w.left {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
        match &w.right {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
    }
    // Adam Optimizer Step function
    fn _step(&mut self, w: &mut Box<Node>, bsz: f64, mut k: usize) -> usize {
        // parameters: 
        let b1: f64 = 0.9;
        let b2: f64 = 0.999;
        let epsilon: f64 = 10.0_f64.powf(-8.0);
        // ------------
        // self.t += 1.0; // can't do this here because it should only happen once per batch, not per every param
        if w.op == "param" {
            let t = self.t;
            let alpha = self.alpha * (1. - b2.powf(t)).sqrt() / (1. - b1.powf(t));
            let prev_m1 = &mut self.prev_m1s[k];
            let prev_m2 = &mut self.prev_m2s[k];
            for e in 0..w.grad.data.len() {
                w.grad.data[e] =  w.grad.data[e] / bsz; // get mean gradient of batch
                //-gradient--clipping-
                if w.grad.data[e] > 1.0 {
                    w.grad.data[e] = 1.0;
                }
                if w.grad.data[e] < -1.0 {
                    w.grad.data[e]=  -1.0;
                }
                //-------------------
                let m1 = prev_m1.data[e];
                let m2 = prev_m2.data[e];
                let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.data[e]);
                let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.data[e].powf(2.));
                w.tensor.data[e] =  w.tensor.data[e]-(alpha*biased_m1/(biased_m2.sqrt()+epsilon));
                prev_m1.data[e] =  biased_m1;
                prev_m2.data[e] = biased_m2;
            }
            k += 1
        }
        match &mut w.left {
            Some(x) => {
                k = self._step(x, bsz, k);
            }
            None => {}
        }
        match &mut w.right {
            Some(x) => {
                k = self._step(x, bsz, k);
            }
            None => {}
        }
        k
    }

    // Manage _step function, which is where the real work happens
    fn step(&mut self, w: &mut Box<Node>) {
        self.t += 1.0;
        self._step(w, w.grad.bsz() as f64, 0);
    }
    // Exponential learning rate decay
    fn exp_lr_decay(&mut self, initial_lr: f64, decay_rate: f64, decay_steps: f64) {
        self.alpha = initial_lr * (decay_rate.powf(self.t/decay_steps))
    }
}

// ----------------------------------- Training Utility Funcs -----------------------------------

fn init_best(best: &mut Vec<Tensor>, w: &Node) {
    if w.op == "param" {
        let p1 = Tensor::zeros(&w.tensor.shape);
        best.push(p1);
    }
    match &w.left {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
    match &w.right {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
}
fn save_best(best: &mut Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        best[k] = w.tensor.copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    k
}
fn load_best(best: & Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        w.tensor = best[k].copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    k
}

// Deprecated - old method of batching
fn init_batch_grads(grads: &mut Vec<Tensor>, w: &Node) {
    if w.op == "param" {
        let p1 = Tensor::zeros(&w.grad.shape);
        grads.push(p1);
    }
    match &w.left {
        Some(x) => {
            init_batch_grads(grads, x);
        }
        None => {}
    }
    match &w.right {
        Some(x) => {
            init_batch_grads(grads, x);
        }
        None => {}
    }
}
fn add_batch_grads(grads: &mut Vec<Tensor>, w: &Node, mut k: usize) -> usize{
    if w.op == "param" {
        // _add_same_size(&mut grads[k], &w.grad);
        grads[k] = _add_same_size_fast(&grads[k], &w.grad);
        k += 1;
   }
   match &w.left {
       Some(x) => {
           k = add_batch_grads(grads, x, k);
       }
       None => {}
   }
   match &w.right {
       Some(x) => {
           k = add_batch_grads(grads, x, k);
       }
       None => {}
   }
   k
}
fn retrive_batch_grads(grads: & Vec<Tensor>, w: &mut Node, mut k: usize) -> usize{
    if w.op == "param" {
        w.grad = grads[k].copy_tens();
        k += 1;
    }
    match &mut w.left {
       Some(x) => {
           k = retrive_batch_grads(grads, x, k);
       }
       None => {}
    }
    match &mut w.right {
       Some(x) => {
           k = retrive_batch_grads(grads, x, k);
       }
       None => {}
    }
    k
}
fn _batch(step: usize, grads: &mut Vec<Tensor>, w: &mut Box<Node>, opt: &mut Adam, bsz: usize, training_set_size: usize, initial_lr: f64) {
    // ASSUMING STEP STARTS AT INDEX 0
    add_batch_grads(grads, w, 0);
    if (step+1) % bsz == 0 {
        retrive_batch_grads(grads, w, 0);
        opt.step(w);
        opt.exp_lr_decay(initial_lr, 0.95, (training_set_size/bsz) as f64);
        for t in 0..grads.len() {
            grads[t] = Tensor::zeros(&grads[t].shape)
        }
    }
}

// Simultaneously shuffle two vectors of tensors. (for shuffling x and y training set)
fn shuffle_xy(x: &mut Vec<Tensor>, y: &mut Vec<Tensor>) {
    let mut rng = rand::thread_rng();
    let len = x.len();
    for i in 0..len {
        let next = rng.gen_range(i..len);
        let tmpx = x[i].copy_tens();
        let tmpy = y[i].copy_tens();
        x[i] = x[next].copy_tens();
        y[i] = y[next].copy_tens();
        x[next] = tmpx;
        y[next] = tmpy;
    }
}

// Make sure current node has correct batch size (useful for when test/val sets have different batch sizes than train set)
fn _match_bsz(mut root: Box<Node>, input_direction: &str) -> Box<Node> {
    match input_direction {
        "left" => {
            let input = root.left.unwrap();
            if root.tensor.bsz() != input.tensor.bsz() {
                root.tensor.shape[0] = input.tensor.shape[0];
                root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
            };
            root.left = Some(input);
        }
        "right" => {
            let input = root.left.unwrap();
            if root.tensor.bsz() != input.tensor.bsz() {
                root.tensor.shape[0] = input.tensor.shape[0];
                root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
            };
            root.left = Some(input);
        }
        _ => panic!("in _match_bsz, use 'left' or 'right' to choose which node to match root bsz with")
    };
    root
}

fn _change_dropout(mut root: Box<Node>, mut drop_prob: f64) -> Box<Node> {
    if root.op == "do" {
        let mut drop_node = root.right.unwrap();
        drop_node.tensor.data[0] = drop_prob;
        root.right = Some(drop_node);
        // if drop_prob != 0.0 {
        //     drop_prob = 0.25;
        // }
    }
    match root.left {
        Some(x) => {
            root.left = Some(_change_dropout(x, drop_prob));
        }
        None => {}
    }
    match root.right {
        Some(x) => {
            root.right = Some(_change_dropout(x, drop_prob));
        }
        None => {}
    }
    root
}

fn validate(mut out: Box<Node>, x_val: Vec<Tensor>, y_val: Vec<Tensor>) -> (Box<Node>,  Vec<Tensor>,  Vec<Tensor>, f64) {
    let mut correct = 0;
    let mut incorrect = 0;
    let mut val_loss = 0.0;
    out = _change_dropout(out, 0.0);
    for t in 0..x_val.len() {
        let x = &x_val[t]; 
        let y = &y_val[t]; 
        out = forward(out, &x);
        // println!("accuracy");
        // println!("out {} | {:?}", out.tensor.data.len(),out.tensor.shape);
        // println!("y {} | {:?}", y.data.len(),y.shape);
        assert!(out.tensor.data.len() == y.data.len());
        for b in 0..out.tensor.bsz(){
            let max_ind = (b*out.tensor.diminc[0]..(b+1)*out.tensor.diminc[0]).reduce(|accum, item| {
                if out.tensor.data[accum] >= out.tensor.data[item] { accum } else { item }
            });
            let y_ind = (b*out.tensor.diminc[0]..(b+1)*out.tensor.diminc[0]).reduce(|accum, item| {
                if y.data[accum] >= y.data[item] { accum } else { item }
            });
            if max_ind == y_ind {
                correct += 1;
            } else {
                incorrect += 1;
            }
        }
        let loss = nll_loss(&mut out, y, true);
        val_loss += loss;
    }
    println!("Validation loss: {} | Total correct: {} | Total incorrect: {}", val_loss / (x_val.len() as f64), correct, incorrect);
    let acc = correct as f64 * 100. / ((correct + incorrect) as f64); 
    println!("Accuracy: {}%", acc);
    (out, x_val, y_val, val_loss)
}
// ----------------------------------- FORWARD AND BACKWARD: -----------------------------------

// These should probably be passing mutable references to the root instead of the root itself.

// Forward Pass function. Recursively finds leaf nodes and gets their values, then performs calculations at each node until it gets back to the root (the output layer).
fn forward(mut root: Box<Node>, f_data: &Tensor) -> Box<Node> {
    // println!("op {} start", root.op);
    if root.op == "input" {
		root.tensor = f_data.copy_tens();
        return root;
	} else if root.op == "leaf" || root.op == "param" {
		return root;
	} else {
		let zeroed_data = Tensor::zeros(&root.tensor.shape);
		root.tensor = zeroed_data;
        // println!("op: {} | shape {:?}", root.op, root.tensor.shape);
	}
	if root.op == "+" { // add
		// Return tensor a + b
        let bias = root.left.unwrap(); 
		let input = root.right.unwrap(); 
        let (bias,input) = rayon::join(|| forward(bias, f_data), || forward(input, f_data));
        root.tensor.shape[0] = input.tensor.shape[0];
        root.tensor = _add_bsz_times(&input.tensor, &bias.tensor); 
        root.left = Some(bias);
        root.right = Some(input);
	} else if root.op == "mm" { // matmul
        let weight1 = root.left.unwrap();
		let input1 = root.right.unwrap();
        let (weight,input) = rayon::join(|| forward(weight1, f_data), || forward(input1, f_data));
        root.tensor.shape[0] = input.tensor.shape[0];
        // println!("r: {:?}", root.tensor.shape);
        // println!("x: {:?}", x.tensor.shape);
        // println!("a: {:?}", a.tensor.shape);

        // let data = &mut root.tensor;
        // println!("a {:?}", a.tensor.shape);
        // println!("x {:?}", x.tensor.shape);
        // println!("root {:?}", data.shape);
		// for k in 0..x.tensor.l2() {
        //     for i in 0..a.tensor.l2() {
        //         for j in 0..a.tensor.l() {
        //             // println!("a: [{},{}], x: [{},{}], root: [{},{}]", i,j,k,j,k,i);
        //             let temp2 = a.tensor.get( &[i,j]) * x.tensor.get( &[k, j]);
        //             let temp = data.get( &[k, i]) + temp2;
        //             data.set( &[k, i], temp);
        //         }
        //     }
		// }
        
        let mut v: Vec<f64> = Vec::new();
        let jj = input.tensor.l(); 
        for b in 0..input.tensor.bsz() { // for each batch
            v.extend::<Vec<f64>>((0..(weight.tensor.l2()*input.tensor.l2())).into_iter().map( |ki| { 
                let i = ki % input.tensor.l2(); // for each vector in input
                let k = (ki - i) / input.tensor.l2(); // for each node
                zip(
                    &input.tensor.data[input.tensor._get_ind(&[b,i,0])..input.tensor._get_ind(&[b,i,jj])], // get the vector of the current batch
                    &weight.tensor.data[weight.tensor._get_ind(&[0,k,0])..weight.tensor._get_ind(&[0,k,jj])] // get the vector of the current node
                ).map(|(a,x)| {a * x}).sum::<f64>()
            }).collect()); 
        }
        root.tensor.data = v;

        root.left = Some(weight);
        root.right = Some(input);
       
	} else if root.op == "sm" { // Log Softmax
		let input = forward(root.left.unwrap(), f_data);
        root.tensor.shape[0] = input.tensor.shape[0];
        root.tensor.data = vec![0.0; input.tensor.data.len()];
		
        // for b in 0..input.tensor.bsz() {
		// 	let mut layer_sum: f64 = 0.0;
		// 	let mut layer_max: f64 = f64::MIN;
		// 	// layer_max is for normalization
        //     for i in 0..input.tensor.l2() {
        //         for j in 0..input.tensor.l() {
        //             if input.tensor.get( &[b,i,j]) > layer_max {
        //                 layer_max = input.tensor.get( &[b,i,j])
        //             }
        //         }
        //     }
        //     for i in 0..input.tensor.l2() {
        //         for j in 0..input.tensor.l() {
        //             layer_sum += (input.tensor.get( &[b,i,j]) - layer_max).exp();
        //         }
        //     }
        //     for i in 0..input.tensor.l2() {
        //         for j in 0..input.tensor.l() {
        //             root.tensor.set( &[b,i,j], input.tensor.get( &[b,i,j]) - layer_sum.ln() - layer_max);
        //             // root.tensor.set( &[b,i,j], input.tensor.get( &[b,i,j]) - layer_max); // I've heard this is acceptable, since exp^max takes up most of probability space, but playing it safe for now.
        //         }
        //     }
		// }

        let mut v: Vec<f64> = Vec::new();
        for b in 0..input.tensor.bsz() {
            let layer_max = input.tensor.data[b*input.tensor.diminc[0]..(b+1)*input.tensor.diminc[0]].iter().reduce(|accum, item| {
                if accum >= item { accum } else { item }
            });
            let layer_sum: f64 = input.tensor.data[b*input.tensor.diminc[0]..(b+1)*input.tensor.diminc[0]].iter().map(|e| {
                (e - layer_max.unwrap()).exp()
            }).sum::<f64>().ln();
            let _v: Vec<f64> = input.tensor.data[b*input.tensor.diminc[0]..(b+1)*input.tensor.diminc[0]].iter().map( |e| {
                e - layer_sum - layer_max.unwrap()
            }).collect();
            v.extend(_v);
        }
        root.tensor.data = v;

        root.left = Some(input);
	} else if root.op == "sig" { // Sigmoid
		let input = forward(root.left.unwrap(), f_data);
        root.tensor.shape[0] = input.tensor.shape[0];
        root.tensor.data = vec![0.0; input.tensor.data.len()];
        for b in 0..input.tensor.bsz() {
            for i in 0..input.tensor.l2() {
                for j in 0..input.tensor.l() {
                    root.tensor.set( &[b,i,j], 1.0/(1.0 + (-1.0*input.tensor.get( &[b,i,j])).exp()));
                }
            }
        }
        root.left = Some(input);
	} else if root.op == "do" {
        let input = forward(root.left.unwrap(), f_data);
        root.tensor.shape[0] = input.tensor.shape[0];
        root.tensor.data = vec![0.0; input.tensor.data.len()];
        let drop_node = root.right.unwrap();
        let mut rng = rand::thread_rng();
		// for i in 0..a.tensor.l2() {
		// 	for j in 0..a.tensor.l() {
        //         let flip = rng.gen_bool(drop_node.tensor.get(&[0]));
        //         if flip == true {
        //             root.tensor.set( &[i, j], 0.0);
        //         } else {
        //             root.tensor.set( &[i, j], a.tensor.get(&[i,j]));
        //         }
		// 	}
		// }
        for i in 0..input.tensor.data.len() {
            let flip = rng.gen_bool(drop_node.tensor.get(&[0]) as f64);
            if flip == true {
                root.tensor.data[i] = 0.0;
            } else {
                root.tensor.data[i] = input.tensor.data[i];
            }
        }
        root.left = Some(input);
        root.right = Some(drop_node);
    } else if root.op == "relu" {
        let input = forward(root.left.unwrap(), f_data);
        let mut relued: Vec<f64> = Vec::new();
        root.tensor.shape[0] = input.tensor.shape[0];
        relued.extend(input.tensor.data.iter().map(|e| {
            if *e < 0.0 {
                0.0
            } else {
                *e
            }}));
        root.tensor.data = relued;
        root.left = Some(input);
    } else if root.op == "c2d" {
        let in1 = root.left.unwrap();
		let ker1 = root.right.unwrap();
        let (input,kernel) = rayon::join(|| forward(in1, f_data), || forward(ker1, f_data));
        
        if root.tensor.bsz() != input.tensor.bsz() {
            root.tensor.shape[0] = input.tensor.shape[0];
            root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
        }        
        // println!("a {:?}", a.tensor.shape);
        // println!("x {:?}", x.tensor.shape);
        // for m in 0..(x.tensor.l()) { // Filter layer
        //     for l in 0..data.shape[0] { // Output dim 0, top index of kernel on input
        //         for k in 0..data.shape[1] { // Output dim 1, left index of kernel on input
        //             for i in 0..x.tensor.shape[0] { // Kernel dim 0
        //                 for j in 0..x.tensor.shape[1] { // Kernel dim 1
        //                     // println!("a: [{},{}], x: [{},{},{}], root: [{},{},{}]", i+l,j+k, i,j, m,l, k, m);
        //                     let temp2 = a.tensor.get(&[i+l,j+k]) * x.tensor.get(&[i, j, m]);
        //                     let temp = data.get(&[l, k, m]) + temp2;
        //                     data.set(&[l, k, m], temp);
        //                 }
        //             }
        //         }
        //     }
        // }


        let mut v: Vec<f64> = Vec::new();
        let mm = kernel.tensor.chans();
        let jj = kernel.tensor.l();
        let ii = kernel.tensor.l2();
        for batch in 0..input.tensor.bsz() { // For each batch b
            for l in 0..root.tensor.l2() { // Output dim 0, top index of kernel on input
                for k in 0..root.tensor.l() { // Output dim 1, left index of kernel on input
                    v.extend::<Vec<f64>>((0..mm).into_iter().map( |m| { // for each filter m
                        (0..ii).into_iter().map(|i| { // for each vector i of current kernel/filter m
                            let itest =  &input.tensor.data[ // for each element corresponding to the current kernel vector
                            input.tensor._get_ind(&[batch,l+i, k]).. 
                            input.tensor._get_ind(&[batch,l+i, k+jj])
                            ];
                            let rtest = &mut kernel.tensor.data[ // for each coefficient in the current kernel vector
                                kernel.tensor._get_ind(&[0,i,0,m])..= // get 0th coefficient of filter m 
                                kernel.tensor._get_ind(&[0,i,jj-1,m]) // get last coefficient of filter m
                            ].iter().step_by(mm); // st
                            assert!(itest.len() == rtest.len());
                            zip(
                                itest,rtest
                                // &input.tensor.data[ // for each element corresponding to the current kernel vector
                                //     input.tensor._get_ind(&[batch,l+i, k]).. 
                                //     input.tensor._get_ind(&[batch,l+i, k+jj])
                                // ], 
                                // &mut kernel.tensor.data[ // for each coefficient in the current kernel vector
                                //     kernel.tensor._get_ind(&[0,i,0,m])..= // get 0th coefficient of filter m 
                                //     kernel.tensor._get_ind(&[0,i,jj-1,m]) // get last coefficient of filter m
                                // ].iter().step_by(mm), // step by len(filter dimension) so you are only getting elements of the current filter
                            ).map(|(a,x)| {a * x}).sum::<f64>() // dot product input and kernel vectors, and sum
                        }).sum() // sum the sums of the vector dot products to get sum of kernel matrix
                    }).collect()); // collect the resulting ouputs for each filter
                } // for each element in vector
            } // for each vector in output
        } // for each batch

        root.tensor.data = v;

        // println!("forward {:?}", root.tensor.data);

        root.left = Some(input);
        root.right = Some(kernel);

    } else if root.op == "reshape" {
        let input = forward(root.left.unwrap(), f_data);
        // if root.tensor.bsz() !=  {
        //     root.tensor.shape[0] = input.tensor.shape[0];
        //     root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
        // }
        // let mut shaper = vec![input.tensor.bsz()];
        // shaper.append(&mut root.tensor.shape.to_vec());
        // println!("{:?}", shaper);
        root.tensor.data = input.tensor.data.to_vec();
        root.tensor.shape[0] = input.tensor.bsz();
        root.left = Some(input);

        let shaper = root.tensor.shape.to_vec();
        root.tensor.reshape(&shaper);
    } else if root.op == "mp" {
        // let a = root.left.unwrap();
        // println!("mp before forward | a shape: {:?} | a datalen: {:?}", a.tensor.shape, a.tensor.data.len());
        // root.left = Some(a);
        let input = forward(root.left.unwrap(), f_data);

        if root.tensor.bsz() != input.tensor.bsz() {
            root.tensor.shape[0] = input.tensor.shape[0];
            root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
        }

        // println!("start of mp | a shape: {:?} | a datalen: {:?}", a.tensor.shape, a.tensor.data.len());
        // println!("start of mp | r shape: {:?} | r datalen: {:?}", root.tensor.shape, root.tensor.data.len());

        let data = &mut root.tensor;
        // let l2dif = a.tensor.shape[0] - data.shape[0];
        // let ldif = a.tensor.l2() - data.l2();
        // for m in 0..a.tensor.l() { // filter layer, left intact
        //     for l in 0..data.shape[0] { // dimension 0 of output
        //         for k in 0..data.l2() { // dimension 1 of output
        //             let mut max = 0.0;
        //             for i in 0..(1+l2dif) { // dimension 0 of maxpool kernel
        //                 for j in 0..(1+ldif) { // dimension 1 of maxpool kernel
        //                     // println!("a: [{},{}], root: [{},{}]", i+ l,j+ k, l, k);
        //                     let temp = a.tensor.get(&[i + l, j+ k, m]);
        //                     if temp > max {
        //                         max = temp;
        //                     }
        //                 }
        //             }
        //             data.set(&[l, k, m], max);
        //         }
        //     }
        // }
        let l2dif = input.tensor.l2() / data.l2();
        let ldif = input.tensor.l() / data.l();
        for b in 0..input.tensor.bsz() {
            for m in 0..input.tensor.chans() { // filter layer, left intact
                for l in 0..data.l2() { // dimension 0 of output
                    for k in 0..data.l() { // dimension 1 of output
                        let mut max = f64::MIN;
                        for i in 0..(l2dif) { // dimension 0 of maxpool kernel
                            for j in 0..(ldif) { // dimension 1 of maxpool kernel
                                let temp = input.tensor._safe_get(&[b,i + l*l2dif, j+ k*ldif, m], "maxpool");
                                if temp > max {
                                    max = temp;
                                }
                            }
                        }
                        data.set(&[b,l, k, m], max);
                    }
                }
            }
        }
        

        // println!("");
        // println!("a tens: {:?}", a.tensor.data);
        // println!("r tens: {:?}", root.tensor.data);
        // println!("");

        root.left = Some(input);

    } else if root.op == "sum_dim" {
        let input = forward(root.left.unwrap(), f_data);
        if root.tensor.bsz() != input.tensor.bsz() {
            root.tensor.shape[0] = input.tensor.shape[0];
            root.tensor.data = vec![0.0; root.tensor.diminc[0]*input.tensor.bsz()];
        }

        // let mm = a.tensor.shape[a.tensor.shape.len()-1];
        // let mut m = 0;
        // let mut summer = 0.0;
        // for i in 0..a.tensor.data.len() {
        //     summer += a.tensor.data[i];
        //     m += 1;
        //     if m+1 == mm {
        //         root.tensor.data[i/(mm-1)] = summer;
        //         m = 0;
        //         summer = 0.0;
        //     }
        // }
        for i in 0..input.tensor.shape[0] {
            for j in 0..input.tensor.shape[1] {
                let mut summer = 0.0;
                for k in 0..input.tensor.shape[2] {
                    summer += input.tensor.get(&[i,j,k]);
                }
                root.tensor.set(&[i,j], summer);
            }
        }
        root.left = Some(input);
    }
    // println!("op {} end", root.op);
    // println!("END op: {} | shape {:?}", root.op, root.tensor.shape);
    root
}

fn backward(mut root: Box<Node>) -> Box<Node>{
    // println!("back {} | shape {:?}", root.op, root.grad.shape);
	if !root.require_grad {
		return root;
	} 
    // check options in left and right 
    match root.left {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.left = Some(x)
        }
    };
    match root.right {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.right = Some(x)
        }
    };
	if root.op == "+" { // add
		// Return tensor a + b
        let mut bias = root.left.unwrap();
		let mut input = root.right.unwrap(); // Assuming right is batched input

        // println!("====");
        // println!("root: {:?}", root.grad.diminc);
        // println!("bias: {:?}", bias.grad.shape);
        // println!("input: {:?}", input.grad.shape);

        // let mut bias_grad: Vec<f64> = Vec::new();
        let bias_grad: Vec<f64> = (0..bias.grad.data.len()).into_iter().map( |j| { // for each coefficient
            root.grad.data[j..].iter().step_by(root.grad.diminc[0]).sum::<f64>() // sum the gradients of that coefficient for every batch
        }).collect(); //.collect_into_vec(&mut bias_grad)); 
        bias.grad.data = bias_grad;


        // can do this without having to specify dimensions
        // if bias.tensor.shape.len() == 3 {
        //     let jj = bias.grad.l(); 
        //     let mut bias_grad: Vec<f64> = Vec::new();
        //     ((0..jj).into_par_iter().map( |j| { // for each coefficient
        //         root.grad.data[j..].iter().step_by(root.grad.l()*root.grad.l2()).sum::<f64>() // sum the gradients of that coefficient for every batch
        //     }).collect_into_vec(&mut bias_grad)); 
        //     bias.grad.data = bias_grad;
        // } else  if bias.tensor.shape.len() == 4 {
        //     let jj = bias.grad.chans(); 
        //     let mut bias_grad: Vec<f64> = Vec::new();
        //     ((0..jj).into_par_iter().map( |j| { // for each coefficient
        //         root.grad.data[j..].iter().step_by(root.grad.l()*root.grad.l2()*root.grad.chans()).sum::<f64>() // sum the gradients of that coefficient for every batch
        //     }).collect_into_vec(&mut bias_grad)); 
        //     bias.grad.data = bias_grad;
        // }

        if input.require_grad {
           input.grad = root.grad.copy_tens();
        }
        let (bias1,input1) = rayon::join(|| backward(bias), || backward(input));
        root.left = Some(bias1);
        root.right = Some(input1);
	} else if root.op == "mm" { // matmul
        let mut weight = root.left.unwrap();
		let mut input = root.right.unwrap();

        // println!("r: {:?}", root.grad.shape);
        // println!("x: {:?}", x.grad.shape);
        // println!("a: {:?}", a.grad.shape);

        // for b in 0..input.tensor.bsz() {
        //     for i in 0..weight.tensor.l2() {
        //         for k in 0..input.tensor.l2() {
        //             for j in 0..weight.tensor.l() {
        //                 let rootgrad = root.grad.get( &[b,k,i]);
        //                 let t1 = weight.grad.get( &[0,i,j]) + (input.tensor.get( &[b,k,j]) * rootgrad);
        //                 let t2 = input.grad.get( &[b,k,j]) + (weight.tensor.get( &[0,i,j]) * rootgrad);
        //                 weight.grad.set( &[0,i,j], t1);
        //                 input.grad.set( &[b,k,j], t2);
        //             }
        //         }
        //     }
        // }

        // I stopped using into_iter() because it seems like normal iterator is faster, but id have tp test more to be sure. (might just be that on smaller sizes, the time it takes to set up parallelism isnt worht the time gained)
        
        let jj = input.tensor.l(); 
        let mut va: Vec<f64> = Vec::new();
        for i in 0..weight.tensor.l2() {
            va.extend::<Vec<f64>>((0..weight.tensor.l()).into_iter().map( |j| { // for each coefficient
                zip(
                    &mut input.tensor.data[j..].iter().step_by(jj), // for each matrix column in each batch, get the element that is multiplied by the current coefficient j
                    &mut root.grad.data[i..].iter().step_by(root.grad.l()) // for each  matrix column in each batch, get the gradient of the current node k
                ).map(|(x,r)| {x * r}).sum()
            }).collect()); 
        }
        weight.grad.data = va;
        if input.require_grad {
            let mut vx: Vec<f64> = Vec::new();
            for b in 0..input.tensor.bsz() {
                for i in 0..input.tensor.l2() {
                    vx.extend::<Vec<f64>>((0..input.tensor.l()).into_iter().map( |j| { // for each element in input
                        zip(
                            &mut weight.tensor.data[j..].iter().step_by(jj), //for each node, get the coefficient for the current element of a
                            & root.grad.data[
                                root.grad._get_ind(&[b, i, 0]).. 
                                root.grad._get_ind(&[b, i, weight.tensor.l2()])
                            ] // also get the gradient of that node for the current batch
                        
                        ).map(|(a,r)| {a * r}).sum()
                    }).collect()); 
                }
            }
            input.grad.data = vx;
        }
        let (weight1,input1) = rayon::join(|| backward(weight), || backward(input));
        root.left = Some(weight1);
        root.right = Some(input1);
	} else if root.op == "sm" { // Log Softmax
		let mut input = root.left.unwrap();
        // for b in 0..root.tensor.bsz() {
        //     for i in 0..root.tensor.l2() {
        //         for j in 0..root.tensor.l() {
        //             input.grad.set( &[b,i,j], -1.0 * (root.grad.get(&[b,i,j]) - root.tensor.get(&[b,i,j]).exp()))
        //         }
        //     }
        // }
        assert!(root.tensor.data.len() == root.grad.data.len());
        let v: Vec<f64> = zip(&root.grad.data, &root.tensor.data).map(|(g,t)| {
            -1.0 * (g - t.exp())
        }).collect();
        input.grad.data = v;
        root.left = Some(backward(input));
	} else if root.op == "sig" { // Sigmoid
		let mut a = root.left.unwrap();
        for b in 0..root.tensor.bsz() {
            for i in 0..root.tensor.l2() {
                for j in 0..root.tensor.l() {
                    a.grad.set( &[b,i,j], root.tensor.get( &[b,i,j]) * (1.0 - root.tensor.get( &[b,i,j])) * root.grad.get( &[b,i,j]));
                }
            }
        }
        root.left = Some(backward(a));
	} else if root.op == "do" || root.op == "relu" {
        let mut a = root.left.unwrap();
        for i in 0..a.tensor.data.len() {
            if a.tensor.data[i] > 0.0 {
                a.grad.data[i] = root.grad.data[i]
            }
        }
        root.left = Some(backward(a));
    } else if root.op == "c2d" {
        let mut input = root.left.unwrap();
        let mut kernel = root.right.unwrap();
        // println!("r: {:?}", root.grad.shape);
        // println!("i: {:?}", input.grad.shape);
        // println!("k: {:?}", kernel.grad.shape);
        // let data = &root.tensor;
        // for m in 0..(x.tensor.l()) { // Filter Layer
        //     for l in 0..data.shape[0] {
        //         for k in 0..data.shape[1] {
        //             for i in 0..x.tensor.shape[0] {
        //                 for j in 0..x.tensor.shape[1] {
        //                     let rootgrad = root.grad.get( &[l, k, m]);
        //                     let t1 = a.grad.get( &[i + l, j + k]) + (x.tensor.get(&[i, j, m]) * rootgrad);
        //                     let t2 = x.grad.get( &[i, j, m]) + (a.tensor.get(&[i + l, j + k]) * rootgrad);
        //                     a.grad.set( &[i + l, j + k], t1);
        //                     x.grad.set( &[i, j, m], t2);
        //                 }
        //             }
        //         }
        //     }
        // }

        let mut input_grad: Vec<f64> = Vec::new();
        let mut kernel_grad: Vec<f64> = Vec::new();
        let mm = kernel.tensor.chans();
        let jj = kernel.tensor.l();
        let ii = kernel.tensor.l2();
        let aj = input.tensor.l();
        let ai = input.tensor.l2();
        let rj = root.tensor.l();
        let ri = root.tensor.l2();
        
        if input.require_grad {
            for b in 0..input.grad.bsz() {
                for a1 in 0..input.grad.l2() {
                    input_grad.extend::<Vec<f64>>((0..input.grad.l()).into_iter().map( |a2| {
                        let mut temp:f64 = 0.0;
                        for i in 0..ii {
                            for j in 0..jj {
                                if a1 >= i && a2 >= j && a1 + ii - (i+1) < ai && a2 + jj - (j+1) < aj {
                                    let ktest = &mut kernel.tensor.data[
                                            kernel.tensor._get_ind(&[0,i,j,0]).. 
                                            kernel.tensor._get_ind(&[0,i,j,mm])
                                        ].iter();
                                    let rtest = &mut root.grad.data[
                                            root.grad._get_ind(&[b,a1 - i,a2 - j,0])..
                                            root.grad._get_ind(&[b,a1 - i,a2 - j,mm])
                                        ].iter();
                                    assert!(ktest.len() == rtest.len());
                                    temp += zip(
                                        ktest, rtest
                                        // &mut kernel.tensor.data[
                                        //     kernel.tensor._get_ind(&[i,j,0]).. 
                                        //     kernel.tensor._get_ind(&[i,j,mm])
                                        // ].iter(),
                                        // &mut root.grad.data[
                                        //     root.grad._get_ind(&[a1 - i,a2 - j,0])..
                                        //     root.grad._get_ind(&[a1 - i,a2 - j,mm])
                                        // ].iter()
                                    ).map(|(x,r)| {x * r}).sum::<f64>();
                                }
                            }
                        }
                        temp
                    }).collect());   
                }
            }
        }
        for i in 0..kernel.tensor.l2() { // Kernel dim 0
            for j in 0..kernel.tensor.l() { // Kernel dim 1
                kernel_grad.extend::<Vec<f64>>((0..mm).into_iter().map( |m| { // for each filter
                    (0..input.tensor.bsz()).into_iter().map( |b| {
                        (i..(ri+i)).into_iter().map( |cur_i| {
                            // println!("i {}",  input.tensor._get_ind(&[b,cur_i,rj+j]) );
                            // println!("r {}", root.tensor._get_ind(&[b,cur_i - i,rj-1,m]));
                            // println!("{} {} {} {}",b,cur_i - i,rj-1,m);
                            
                            let itest = &mut input.tensor.data[
                                input.tensor._get_ind(&[b,cur_i,j]).. // first element in input that was multiplied by current coefficient
                                input.tensor._get_ind(&[b,cur_i,rj+j]) // last element in input that was multiplied by current coefficient
                                ].iter(); 
                            let rtest = &mut root.grad.data[
                                root.tensor._get_ind(&[b,cur_i - i,0,m])..= //i
                                root.tensor._get_ind(&[b,cur_i - i,rj-1,m]) //i
                            ].iter().step_by(mm);
                            // println!("i {} | r {}", itest.len() , rtest.len());
                            // println!("i {:?} | r {:?}", input.grad.shape , root.grad.shape);
                            // println!("i {:?} | r {:?}", input.grad.data.len() , root.grad.data.len());
                            assert!(itest.len() == rtest.len());
                            zip(
                                itest, rtest
                                // &mut input.tensor.data[
                                //     input.tensor._get_ind(&[b,cur_i,j]).. // first element in input that was multiplied by current coefficient
                                //     input.tensor._get_ind(&[b,cur_i,rj+j]) // last element in input that was multiplied by current coefficient
                                //     ].iter(), 
                                // &mut root.grad.data[m..].iter().step_by(mm), // for current filter, get every element in root that was result of current coefficient * current input element
                            ).map(|(a,r)| {a * r}).sum::<f64>()
                        }).sum::<f64>()
                    }).sum::<f64>()
                }).collect()); 
            }
        }

        // zip(
        //     &mut input.tensor.data[
        //         input.tensor._get_ind(&[0,i,j])..= // first element in input that was multiplied by current coefficient
        //         input.tensor._get_ind(&[input.tensor.bsz()-1,ai-ii+i,aj-jj+j]) // last element in input that was multiplied by current coefficient
        //         ].iter(), 
        //     &mut root.grad.data[m..].iter().step_by(mm), // for current filter, get every element in root that was result of current coefficient * current input element
        //     // [input.diminc[0]*b + m..]
        // ).map(|(a,r)| {a * r}).sum()

        if input.require_grad {
            input.grad.data = input_grad;
        }
        kernel.grad.data = kernel_grad;

        // println!("back {:?}", kernel.grad.data);
        
        let (a1,x1) = rayon::join(|| backward(input), || backward(kernel));
        root.left = Some(a1);
        root.right = Some(x1);
    } else if root.op == "reshape" {
        let mut input = root.left.unwrap();
        if input.require_grad {
            // input.grad = root.grad.copy_tens();
            // input.grad.reshape(&input.tensor.shape);
            input.grad.data = root.grad.data.to_vec();
        }
        root.left = Some(backward(input));
    } else if root.op == "mp" {
        let mut input = root.left.unwrap();
        let data = &root.tensor;
        // let l2dif = a.tensor.shape[0] - data.shape[0];
        // let ldif = a.tensor.l2() - data.l2();
        // for m in 0..a.tensor.l() {
        //     for l in 0..data.shape[0] {
        //         for k in 0..data.l2() {
        //             let mut max = 0.0;
        //             let mut maxind = [0,0,0];
        //             for i in 0..(1 + l2dif) {
        //                 for j in 0..(1+ ldif) {
        //                     // println!("a: [{},{}], root: [{},{}]", i+ l,j+ k, l, k);
        //                     let temp = a.tensor.get(&[i + l, j+ k, m]);
        //                     if temp > max {
        //                         max = temp;
        //                         maxind = [i + l, j+ k, m];
        //                     }
        //                 }
        //             }
        //             a.grad.set(&maxind, root.grad.get(&[l,k]));
        //         }
        //     }
        // }

        // let l2dif = input.tensor.shape[0] / data.shape[0];
        // let ldif = input.tensor.l2() / data.l2();
        // for m in 0..input.tensor.l() { // filter layer, left intact
        //     for l in 0..data.shape[0] { // dimension 0 of output
        //         for k in 0..data.l2() { // dimension 1 of output
        //             let mut max = f64::MIN;
        //             let mut maxind = [0,0,0];
        //             for i in 0..(l2dif) { // dimension 0 of maxpool kernel
        //                 for j in 0..(ldif) { // dimension 1 of maxpool kernel
        //                     let temp = input.tensor.get(&[i + l*l2dif, j+ k*ldif, m]);
        //                     if temp > max {
        //                         max = temp;
        //                         maxind = [i + l*l2dif, j+ k*ldif, m];
        //                     }
        //                 }
        //             }
        //             input.grad.set(&maxind, root.grad.get(&[l,k,m]));
        //         }
        //     }
        // }
        let l2dif = input.tensor.l2() / data.l2();
        let ldif = input.tensor.l() / data.l();
        for b in 0..input.tensor.bsz() {
            for m in 0..input.tensor.chans() { // filter layer, left intact
                for l in 0..data.l2() { // dimension 0 of output
                    for k in 0..data.l() { // dimension 1 of output
                        let mut max = f64::MIN;
                        let mut maxind = [0,0,0,0];
                        for i in 0..(l2dif) { // dimension 0 of maxpool kernel
                            for j in 0..(ldif) { // dimension 1 of maxpool kernel
                                let temp = input.tensor._safe_get(&[b,i + l*l2dif, j+ k*ldif, m], "maxpool");
                                if temp > max {
                                    max = temp;
                                    maxind = [b, i + l*l2dif, j+ k*ldif, m];
                                }
                            }
                        }
                        input.grad.set(&maxind, root.grad.get(&[b,l,k,m]));
                    }
                }
            }
        }
        // println!("");
        // println!("a: {:?}", a.grad.data);
        // println!("r: {:?}", root.grad.data);
        // println!("");

        root.left = Some(backward(input));
    } else if root.op == "sum_dim" {
        let mut a = root.left.unwrap();
        // let mm = a.tensor.shape[a.tensor.shape.len()-1];
        // for g in 0..root.grad.data.len() {
        //     for m in 0..mm {
        //         a.grad.data[mm*g + m] = root.grad.data[g]
        //     }
        // }
        for i in 0..a.tensor.shape[0] {
            for j in 0..a.tensor.shape[1] {
                let g = root.grad.get(&[i,j]);
                for k in 0..a.tensor.shape[2] {
                    a.tensor.set(&[i,j,k], g);
                }
            }
        }
        root.left = Some(backward(a));
    }
    root
}

// ----------------------------------- TESTING (will be removed when done) -----------------------------------

fn _simple(shaper: &Vec<usize>) -> Box<Node> {
    let x_placeholder = Tensor::zeros(shaper);
	let mut x_node = leaf(x_placeholder, false);
    x_node.op = "input";
    let _bsz = shaper[0];

    // let l1 = linear(x_node, 10, "he");
    // log_softmax(l1)

    // let l1 = linear(x_node, 100, "xavier");
    // let s1 = sigmoid(l1);
    // let l2 = linear(s1, 10, "xavier");
    // log_softmax(l2)
    
    // let r = reshape_node(x_node, vec![28,28]);
    // let c = conv2d(r, vec![28,28], 40, "xavier");
    // let cc = reshape_node(c, vec![1,40,1]);
    // let m = max_pool(cc, vec![1,4]);
    // let f = reshape_node(m, vec![1,10]);
    // log_softmax(f)

    // let c = conv2d(x_node, vec![1,784], 100, "xavier");
    // let f = reshape_node(c, vec![1,100]);
    // let s = sigmoid(f);
    // let c2 = conv2d(s, vec![1,100], 10, "xavier");
    // let f2 = reshape_node(c2, vec![1,10]);
    // log_softmax(f2)

    // let r1 = reshape_node(x_node, vec![28,28]);
    // let c1 = conv2d(r1, vec![3,3], 32, "he");
    // let re1 = relu(c1);
    // let c2 = conv2d(re1, vec![3,3], 64, "he");
    // let re2 = relu(c2);
    // let m1 = max_pool(re2, vec![2,2]);
    // // let d1 = dropout(m1, 0.25);
    // let r2 = flatten(m1);
    // // let l1 = linear(r2, 128, "he");
    // let re3 = relu(r2);
    // // let d2 = dropout(re3, 0.5);
    // let l2 = linear(re3, 10, "xavier");
    // log_softmax(l2)

    // // add dropout
    let r = reshape_node(x_node, vec![28,28]);
    let c = conv2d(r, vec![5,5], 32, "he");
    let r1 = relu(c);
    let m = max_pool(r1, vec![2,2]);
    let f = flatten(m);
    let d = dropout(f, 0.1);
    let l1 = linear(d, 100, "xavier");
    let s1 = sigmoid(l1);
    let l2 = linear(s1, 10, "xavier");
    log_softmax(l2)
}

pub(crate) fn simple() -> std::io::Result<()> {
    let dataset_size = 60000; //51200; // default: 51200
    let lr = 0.001; // default: 0.001
    let epochs = 20;
    let bsz = 120; // each element in training set is a batch of inputs.

// ========== read in data =============================
    let file = File::open("fashion-mnist_train.csv")?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut data: Vec<csv::StringRecord> = Vec::new();
    for result in rdr.records() {
        let record = result?;
        data.push(record);
    }

    let file2 = File::open("fashion-mnist_test.csv")?;
    let mut rdr2 = csv::Reader::from_reader(file2);
    let mut data2: Vec<csv::StringRecord> = Vec::new();
    for result in rdr2.records() {
        let record = result?;
        data2.push(record);
    }

    let mut x_train = Vec::new();
    let mut y_train = Vec::new();
    let mut last_ind = 0;
    for b in 0..(dataset_size / bsz) {
        // let data_slice = &data[b];
        let mut x = Tensor::zeros(&vec![bsz,1,784]);
        let mut y = Tensor::zeros(&vec![bsz,1,10]);
        for e in 0..x.bsz() {
            let data_slice = &data[b*x.bsz() + e];
            y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
            for i in 1..(x.l()+1) {
                x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
            }
        }
        x_train.push(x);
        y_train.push(y);
        last_ind += 1;
    }

    // let mut x_test = Vec::new();
    // let mut y_test = Vec::new();
    // for b in 0..((x_train.len()/10+1)) { // /bsz
    //     // let data_slice = &data[b + last_ind];
    //     let mut x = Tensor::zeros(&vec![1,1,784]);
    //     let mut y = Tensor::zeros(&vec![1,1,10]);
    //     for e in 0..x.bsz() {
    //         let data_slice = &data[(last_ind+b)*x.bsz() + e];
    //         y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
    //         for i in 1..(x.l()+1) {
    //             x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
    //         }
    //     }
    //     x_test.push(x);
    //     y_test.push(y);
    // }

    let mut x_val = Vec::new();
    let mut y_val = Vec::new();
    for b in 0..(10000) {
        // let data_slice = &data2[b];
        let mut x = Tensor::zeros(&vec![1,1,784]);
        let mut y = Tensor::zeros(&vec![1,1,10]);
        for e in 0..x.bsz() {
            let data_slice = &data2[b*x.bsz() + e];
            y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
            for i in 1..(x.l()+1) {
                x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
            }
        }
        x_val.push(x);
        y_val.push(y);
    }
// =========================================================


    let mut out = _simple(&x_train[0].shape);
    let mut opt = Adam {
        t: 0.0, 
        alpha: lr, 
        prev_m1s: Vec::new(),
        prev_m2s: Vec::new(),
    };
    opt.init_prevs(&out); //out.as_ref()
    let now_total = Instant::now();
    let mut best_epoch = 0;
    let mut best_epoch_loss = 999999.9;
    let mut best_params: Vec<Tensor> = Vec::new();
    init_best(best_params.as_mut(), out.as_ref());
    // let mut batch_grads: Vec<Tensor> = Vec::new();
    // init_batch_grads(&mut batch_grads, &out);
    println!("Starting at {:?}", Utc::now());
    println!("======= START TRAINING =======");
    for epoch in 0..epochs {
        shuffle_xy(&mut x_train, &mut y_train);
        let train_bar = ProgressBar::new(x_train.len() as u64);
        train_bar.set_style(ProgressStyle::with_template("{msg} | {bar:40.cyan/blue} {pos:>7}/{len:7} {elapsed_precise}")
            .unwrap().progress_chars("##-"));
        let epoch_label = "Epoch ".to_owned() + &epoch.to_string();
        train_bar.set_message(epoch_label);
        let mut total_loss = 0.0;
        // let now = Instant::now();
        for i in 0..x_train.len() {
            let x = &x_train[i]; 
            let y = &y_train[i];
            out = forward(out, x);
            total_loss += nll_loss(&mut out, y, false);
            out = backward(out);
            opt.step(&mut out);
            opt.exp_lr_decay(lr, 0.95, (x_train.len()* epochs) as f64);
            train_bar.inc(1);
            // let cur_time = now.elapsed().as_secs().to_string();
            // train_bar.set_prefix(cur_time);


            // if i == x_train.len() - 1  && epoch > 50{  // && epoch == epochs - 1
            //     let mut pred = out.tensor.copy_tens();
            //     for p in 0..pred.l() {
            //         pred.data[p] = (((pred.data[p]).exp() * 100.0) as u128) as f64 / 100.0;
            //     }
            //     println!(" y    | {:?}", &y.data);
            //     println!(" pred | {:?}", pred.data);
            // }
        }
        // let elapsed_time = now.elapsed().as_secs();
        // let extra_seconds = elapsed_time % 60;
        // let mins = elapsed_time / 60;
        
        train_bar.finish();
        out = _change_dropout(out, 0.0);

        // for t in 0..x_train.len() {
        //     let x = &x_train[t]; 
        //     let y = &y_train[t]; 
        //     out = forward(out, &x);
        //     let loss = nll_loss(&mut out, y, true);
        //     total_loss += loss;
        // }
        // println!("Epoch {} | {} | {} minutes {} seconds", epoch, total_loss / (x_train.len() as f64), mins, extra_seconds);
        println!("Train Loss: {} ",total_loss / (x_train.len() as f64));
        (out, x_val, y_val, total_loss) = validate(out, x_val, y_val);
        
        // let mut test_loss = 0.0;
        // for t in 0..x_test.len() {
        //     let x = &x_test[t]; 
        //     let y = &y_test[t]; 
        //     out = forward(out, &x);
        //     let loss = nll_loss(&mut out, y, true);
        //     test_loss += loss;
        // }

        // println!("Epoch {} | {} | {} minutes {} seconds", epoch, test_loss / (x_test.len() as f64), mins, extra_seconds);
        // println!("Test Loss:  {} ",test_loss / (x_test.len() as f64));
        // println!("  Train Loss: {}   |   Test Loss:  {} ",total_loss / (x_train.len() as f64), test_loss / (x_test.len() as f64));


        out = _change_dropout(out, 0.1);

        if total_loss < best_epoch_loss {
            best_epoch_loss = total_loss;
            best_epoch = epoch;
            save_best(best_params.as_mut(), out.as_mut(), 0);
        }
    }
   
    let total_elapsed = now_total.elapsed().as_secs();
    let tot_extra_seconds = total_elapsed % 60;
    let tot_mins = total_elapsed / 60;
    println!("Total training time: {} minutes {} seconds", tot_mins, tot_extra_seconds);
    // println!("Best Epoch: {} | {}", best_epoch, best_epoch_loss / (x_test.len() as f64));
    println!("Best Epoch: {} | {}", best_epoch, best_epoch_loss / (x_val.len() as f64));
    let now_val = Instant::now();
    println!("=====VALIDATING======");
    load_best(best_params.as_ref(), out.as_mut(), 0);
    let mut correct = 0;
    let mut incorrect = 0;
    let mut val_loss = 0.0;
    out = _change_dropout(out, 0.0);
    for t in 0..x_val.len() {
        let x = &x_val[t]; 
        let y = &y_val[t]; 
        out = forward(out, &x);
        assert!(out.tensor.data.len() == y.data.len());
        for b in 0..out.tensor.bsz(){
            let max_ind = (b*out.tensor.diminc[0]..(b+1)*out.tensor.diminc[0]).reduce(|accum, item| {
                if out.tensor.data[accum] >= out.tensor.data[item] { accum } else { item }
            });
            let y_ind = (b*out.tensor.diminc[0]..(b+1)*out.tensor.diminc[0]).reduce(|accum, item| {
                if y.data[accum] >= y.data[item] { accum } else { item }
            });
            if max_ind == y_ind {
                correct += 1;
            } else {
                incorrect += 1;
            }
        }
        let loss = nll_loss(&mut out, y, true);
        val_loss += loss;
    }
    println!("Validation | {} | {:#?}", val_loss / (x_val.len() as f64), now_val.elapsed());
    println!("Total correct:   {}", correct);
    println!("Total incorrect: {}", incorrect);
    let acc = correct * 100 / (correct + incorrect);// as f64 / 100.0;
    println!("Accuracy: {}%", acc);
    let acc = correct as f64 * 100. / (correct + incorrect) as f64;
    println!("Accuracy: {}%", acc);

    Ok(())
}

// pub(crate) fn _c2d_test() {
//     let mut a = Tensor::he(&vec![3,3]);
//     let b = Tensor::ones(&vec![3,3]);
//     a.set(&[0,0], 2.);
//     a.set(&[0,1], 1.);
//     a.set(&[1,0], 1.);
//     a.set(&[1,1], 1.);
//     a.set(&[0,2], 0.);
//     a.set(&[1,2], 1.);
//     a.set(&[2,0], 0.);
//     a.set(&[2,2], 1.);
//     a.set(&[2,1], 3.);
//     a.reshape(&vec![1,9]);
//     println!("a: {:#?}", a.data);
//     // let mut x = Tensor::xavier(& [2,2]);
//     let in1 = leaf(a, false);
//     let in2 = reshape_node(in1, vec![3,3]);
//     let mut l1 = conv2d(in2, vec![2,2],1, "xavier");
//     let mut x = l1.right.unwrap();
//     x.tensor.set(&[0,0], 0.);
//     x.tensor.set(&[0,1], 1.);
//     x.tensor.set(&[1,0], 2.);
//     x.tensor.set(&[1,1], 3.);
//     l1.right = Some(x);
//     let l1 = forward(l1, &b);
//     println!("l1: {:#?}", l1.tensor.data);
//     let l1 = backward(l1);
//     let x = l1.right.unwrap();
//     println!("l1: {:#?}", x.grad.data);

// }
