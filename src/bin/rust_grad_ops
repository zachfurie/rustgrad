#![allow(dead_code)]

use std::{fs::File, time::{Instant}, iter::zip};
use rand::prelude::*;
use csv;
use rand_distr;
use rayon::prelude::*;
use chrono::prelude::*;


// ----------------------------------- Notes:  ----------------------------------------------

// Need to reset random seed
// implement parallelism
// Maybe use mutable arrays instead of vecs
// dont think i need separate tensor structs for data and gradients since they will always be the same shape.

// RELU appears to be broken right now.

//  ----------------------------------- Data Structures:  -----------------------------------

struct Node {
    left: Option<Box<Node>>,
	right: Option<Box<Node>>,
	op: &'static str,
	tensor: Tensor,
	grad: Tensor,
	require_grad: bool,
}

struct Tensor {
	data:  Vec<f64>, // Float64 all tensor data in a single slice 
	shape: Vec<usize>, // Int  [i] = length of dimension i
    diminc: Vec<usize>,
}

//  ----------------------------------- Auxiliary Functions:  -----------------------------------

impl Tensor {
    fn zeros(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:   vec![0.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        }
    }

    fn ones(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs
        }
    }

    fn xavier(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        // let std = (shaper[shaper.len()-1] as f64).sqrt(); // This will not be correct for cases like conv2d
        let std = ((shaper[0]*shaper[1]) as f64).sqrt(); // for conv2d, but will not work on batched linear
        let mut rng = rand::thread_rng();
        let mut t = Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        };
        for i in 0..data_len {
            let y: f64 = rng.gen(); // generates a float between 0 and 1
            if rng.gen_bool(0.5) {
                t.data[i] = y / std; // generates a float between 0 and 1
            } else {
                t.data[i] = -1. * y / std; // generates a float between 0 and 1
            }
        };
        t
    }

    fn he(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        // let std = (2. / (shaper[shaper.len()-1] as f64)).sqrt(); // This will not be correct for cases like conv2d
        let std = (2. / ((shaper[0]*shaper[1]) as f64)).sqrt(); // for conv2d, but will not work on batched linear
        let mut rng = rand::thread_rng();
        let mut t = Tensor {
            data:   vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        };
        for i in 0..data_len {
            let y: f64 = rng.sample(rand_distr::StandardNormal) ; // generates a float between 0 and 1
            t.data[i] = y * std; // generates a float between 0 and 1
        };
        t
    }

    fn get(&self, ind: &[usize]) -> f64 {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index]
    }

    fn _safe_get(&self, ind: &[usize], info: &str) -> f64 {
        let mut index = 0;
        if ind.len() > self.diminc.len() {
            panic!("ERROR: index {:?} is invalid for tensor with shape {:?}. \n Info: {:?}", ind, self.shape, info);
        } 
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        if index >= self.data.len() {
            panic!("ERROR: index {:?} is outside of tensor with shape {:?}. \n Info: {:?}", ind, self.shape, info);
        } 
        self.data[index]
    }

    fn _get_ind(&self, ind: &[usize]) -> usize {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        index
    }
    
    fn set(&mut self, ind: &[usize], elem: f64) {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index] = elem;
    }

    // Assuming data is a 2d matrix, get length of outer dimension
    fn l2(&self) -> usize {
        // self.shape[self.shape.len()-2]
        self.shape[1]
    }

    // Assuming data is a 2d matrix, get length of inner dimension
    fn l(&self) -> usize {
        // self.shape[self.shape.len()-1]
        self.shape[2]
    }

    // Get length of batch dimension
    fn bsz(&self) -> usize {
        self.shape[0]
    }

    // Get length of channels/filters dimension 
    fn chans(&self) -> usize {
        if self.shape.len() < 4 {
            panic!("Tensor of shape {:?} does not have channels", self.shape);
        }
        self.shape[3]
    }

    fn copy_tens(&self) -> Tensor {
        let new_shape = self.shape.to_vec();
        let new_data = self.data.to_vec();
        let new_incs = self.diminc.to_vec();
        Tensor {
            data:  new_data, // all tensor data in a single slice 
            shape: new_shape, // [i] = length of dimension i
            diminc: new_incs,
        }
    }

    fn reshape(&mut self, shaper: &Vec<usize>) {
        let mut data_len: usize = 1;
        let mut incs =  vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        if data_len != self.data.len() {
            panic!("Shape {:?} is not compatible with Tensor of shape {:?}", shaper, self.shape);
        }
        self.shape = shaper.to_vec();
        self.diminc = incs;
    }
}

fn _add_same_size(dst: &mut Tensor, t: &Tensor ) {
    for (i,x) in t.data.iter().enumerate() {
        dst.data[i] += x
    }
}

fn _add_same_size_fast(a: &Tensor, b: &Tensor ) -> Tensor {
    let f: Vec<f64> = a.data.iter().zip(b.data.iter()).map(|(&aa, &bb)| aa + bb).collect();
    Tensor {
        data: f,
        shape: a.shape.to_vec(),
        diminc: a.diminc.to_vec(),
    }
}

fn _add_bsz_times(a: &Tensor, b: &Tensor ) -> Tensor {
    let mut f: Vec<f64> = Vec::new();
    for batch in 0..a.bsz() {
        f.extend::<Vec<f64>>(zip(
            a.data[a._get_ind(&[batch,0,0])..a._get_ind(&[batch+1,0,0])].iter(),
            b.data.iter(),
        ).map(|(&aa, &bb)| {aa + bb}).collect());
    }
    Tensor {
        data: f,
        shape: a.shape.to_vec(),
        diminc: a.diminc.to_vec(),
    }
}
//  ----------------------------------- BASIC OPS:  -----------------------------------

// Inputs, weights, etc
fn leaf(tens: Tensor, require_grad: bool) -> Box<Node> {
	let zero_grad = Tensor::ones(&tens.shape);
	Box::new(Node {
        left: None,
        right: None, 
        op: "leaf", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad
    })
}

// Syntactic sugar for leaf nodes that are weights
fn weight(shaper: Vec<usize>) -> Box<Node> {
	// let tens = Tensor::xavier(&shaper);
    let tens = Tensor::he(&shaper);
    let zero_grad = Tensor::ones(&shaper);
	Box::new(Node {
        left: None,
        right: None, 
        op: "param", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Add two identically shaped tensors
fn add(node1: Box<Node>, node2: Box<Node>) -> Box<Node> { //assuming right is batched input 
	let zero_grad = Tensor::ones(&node2.tensor.shape);
	let data = Tensor::zeros(&node2.tensor.shape);
	Box::new(Node{
        left: Some(node1), 
        right: Some(node2), 
        op: "+", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Matrix Multiplication
fn matmul(a: Box<Node>, x: Box<Node>) -> Box<Node> {
    assert!(x.tensor.l2() == 1);
	let zero_grad = Tensor::ones(&vec![x.tensor.bsz(), 1, a.tensor.l2()]);
	let data = Tensor::zeros(&vec![x.tensor.bsz(), 1, a.tensor.l2()]);
	Box::new(Node{
        left: Some(a), 
        right: Some(x), 
        op: "mm",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

fn reshape_node(inn: Box<Node>, shaper: Vec<usize>) -> Box<Node> {
	let zero_grad = Tensor::ones(&shaper);
	let data = Tensor::zeros(&shaper);
    let rq = inn.require_grad;
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "reshape", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: rq,
    })
}

fn _batch_norm(a: Box<Node>)-> Box<Node> {
    let zero_grad = Tensor::ones(&a.grad.shape);
	let data = Tensor::zeros(&a.tensor.shape);
    let rq = a.require_grad;
	Box::new(Node{
        left: Some(a), 
        right: None, 
        op: "bn",
        tensor: data, 
        grad: zero_grad, 
        require_grad: rq,
    })
}

//  ----------------------------------- ML OPS:  -----------------------------------

// Linear layer. Returns output, weight, bias so that weight and bias can be put in params[].
fn linear(inn: Box<Node>, out_dim: usize) -> Box<Node> { //  Node<'a>,  Node<'a>) {
	let l_weight = weight(vec![1,out_dim,inn.tensor.l()]);
	let bias = weight(vec![1, 1, out_dim]);
	let in_times_weight = matmul(l_weight,inn);
	let plus_bias = add(bias, in_times_weight);
	plus_bias
}

// Log Softmax layer
fn log_softmax(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sm", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Sigmoid layer
fn sigmoid(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sig", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Dropout 
fn dropout(inn: Box<Node>, drop_prob: f64) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
    let mut dropout_tens = Tensor::zeros(&vec![1]);
    dropout_tens.set(&[0], drop_prob);
    let dropout_node = Box::new(Node {
        left: None, 
        right: None, 
        op: "leaf", 
        tensor: dropout_tens, 
        grad: Tensor::zeros(&vec![1]), 
        require_grad: false,
    });
	Box::new(Node {
        left: Some(inn), 
        right: Some(dropout_node), 
        op: "do", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Conv2D - NOTE: conv2d currently works for 2d input OR 3d input where last dimension is channels/filters. DOES NOT WORK with batched inputs!
fn conv2d(a: Box<Node>, conv_size: Vec<usize>, filters: usize) -> Box<Node> {
    let x = weight(vec![conv_size[0], conv_size[1], filters]);
    let zero_grad = Tensor::ones(&vec![1 + a.tensor.shape[0] - x.tensor.shape[0], 1 + a.tensor.shape[1] - x.tensor.shape[1], filters]);
	let data = Tensor::zeros(&vec![1 + a.tensor.shape[0] - x.tensor.shape[0], 1 + a.tensor.shape[1] - x.tensor.shape[1], filters]);
	let ll = a.tensor.shape.to_vec();
    let dl = data.shape.to_vec();
    let mut cv_layer = Box::new(Node{
        left: Some(a), 
        right: Some(x), 
        op: "c2d",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    });
    if ll.len() == 3 {
        let a = cv_layer.left.unwrap();
        let sd = _sum_dim(a);
        cv_layer.left = Some(sd);
    } 
    // println!("cv: {:?}", cv_layer.grad.shape);
    let bias = weight(dl);
    let plus_bias = add(cv_layer, bias);
	plus_bias
}

// Helper for conv2d
fn _sum_dim(a: Box<Node>) -> Box<Node> {
    let new_shape = vec![a.tensor.shape[0], a.tensor.shape[1]];
	let zero_grad = Tensor::ones(&new_shape);
	let data = Tensor::zeros(&new_shape);
    let rq = a.require_grad;
	Box::new(Node{
        left: Some(a), 
        right: None, 
        op: "sum_dim",
        tensor: data, 
        grad: zero_grad, 
        require_grad: rq,
    })
}

// ReLU
fn relu(a: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&a.grad.shape);
	let data = Tensor::zeros(&a.tensor.shape);
	Box::new(Node{
        left: Some(a), 
        right: None, 
        op: "relu",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Max Pooling 2D
fn max_pool(inn: Box<Node>, shaper: Vec<usize>) -> Box<Node> {
	let zero_grad = Tensor::ones(&vec![inn.tensor.shape[0] / shaper[0], inn.tensor.l2() / shaper[1], inn.tensor.l()]);
	let data = Tensor::zeros(&vec![inn.tensor.shape[0] / shaper[0], inn.tensor.l2() / shaper[1], inn.tensor.l()]);
    Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "mp", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// take 3 dimensional input, flatten to 1 dimensional vector (actually 2d with 0-dimension of length 1)
fn flatten(inn: Box<Node>) -> Box<Node> {
    let shaper = &inn.tensor.shape;
    let bigshape = shaper.iter().product();
    reshape_node(inn, vec![1, bigshape])
}

//  ----------------------------------- LOSS AND OPTIM:  -----------------------------------

// Negative Log Likelihood. Takes prediction tensor and target tensor as inputs, returns loss, and updates gradients from input
fn nll_loss(pred: &mut Box<Node>, target: &Tensor) -> f64 { 
    // println!("pred {} | {:?}", pred.tensor.data.len(), pred.tensor.shape);
    // println!("target {} | {:?}", target.data.len(), target.shape);
	let mut loss = 0.0;
	for b in 0..target.bsz() {
        for j in 0..target.l2() {
            for i in 0..target.l() {
                let t = target.get(&[b,j,i]);
                loss -= t * pred.tensor.get(&[b,j,i]);
                pred.grad.set(&[b,j,i], t);
            }
        }
    }
    loss  / (target.bsz() as f64)
}

// initialize adam optimizer
struct Adam {
	// b1 float64, b2 float64
	t:        f64,// init to 0
	alpha:    f64,
	prev_m1s: Vec<Tensor>,
	prev_m2s: Vec<Tensor>,
}

impl Adam {
// Adam Optimizer Step function
    fn init_prevs(&mut self, w: &Node) {
        if w.op == "param" {
            let p1 = Tensor::zeros(&w.grad.shape);
            let p2 = Tensor::zeros(&w.grad.shape);
            self.prev_m1s.push(p1);
            self.prev_m2s.push(p2);
        }
        match &w.left {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
        match &w.right {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
    }
    //create prev_m1s and prev_m2s with same recursive search pattern (DFS)
    fn step(&mut self, w: &mut Node, bsz: f64, mut k: usize) -> usize {
        // parameters: would normally get these from adam_init, but keeping them here as defaults for now.
        let b1: f64 = 0.9;
        let b2: f64 = 0.999;
        let epsilon: f64 = 10.0_f64.powf(-8.0);
        // ------------
        self.t += 1.0;
        if w.op == "param" {
            let t = self.t;
            let alpha = self.alpha * (1. - b2.powf(t)).sqrt() / (1. - b1.powf(t));
            let prev_m1 = &mut self.prev_m1s[k];
            let prev_m2 = &mut self.prev_m2s[k];
            // println!("m1 {:?}", prev_m1.shape);
            // println!("m2 {:?}", prev_m2.shape);
            // println!("grad {:?}", w.grad.shape);
            // if w.grad.shape.len() == 2 {
            //     for i in 0..w.grad.shape[0] {
            //         for j in 0..w.grad.shape[1] {
            //             w.grad.set( &[i,j], w.grad.get( &[i,j])/bsz); // get mean gradient of batch
            //             //-gradient--clipping-
            //             if w.grad.get( &[i,j]) > 1.0 {
            //                 w.grad.set( &[i,j], 1.0);
            //             }
            //             if w.grad.get( &[i,j]) < -1.0 {
            //                 w.grad.set( &[i,j], -1.0);
            //             }
            //             //-------------------
            //             let m1 = prev_m1.get( &[i,j]);
            //             let m2 = prev_m2.get( &[i,j]);
            //             let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.get( &[i,j]));
            //             let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.get( &[i,j]).powf(2.));
            //             w.tensor.set( &[i,j], w.tensor.get( &[i,j])-(alpha*biased_m1/(biased_m2.sqrt()+epsilon)));
            //             prev_m1.set( &[i,j], biased_m1);
            //             prev_m2.set( &[i,j], biased_m2);
            //         }
            //     }
            // } else if w.grad.shape.len() == 3 {
            //     for k in 0..w.grad.shape[0] {
            //         for i in 0..w.grad.shape[1] {
            //             for j in 0..w.grad.shape[2] {
            //                 w.grad.set( &[k,i,j], w.grad.get( &[k,i,j])/bsz); // get mean gradient of batch
            //                 //-gradient--clipping-
            //                 if w.grad.get( &[k,i,j]) > 1.0 {
            //                     w.grad.set( &[k,i,j], 1.0);
            //                 }
            //                 if w.grad.get( &[k,i,j]) < -1.0 {
            //                     w.grad.set( &[k,i,j], -1.0);
            //                 }
            //                 //-------------------
            //                 let m1 = prev_m1.get( &[k,i,j]);
            //                 let m2 = prev_m2.get( &[k,i,j]);
            //                 let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.get( &[k,i,j]));
            //                 let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.get( &[k,i,j]).powf(2.));
            //                 w.tensor.set( &[k,i,j], w.tensor.get( &[k,i,j])-(alpha*biased_m1/(biased_m2.sqrt()+epsilon)));
            //                 prev_m1.set( &[k,i,j], biased_m1);
            //                 prev_m2.set( &[k,i,j], biased_m2);
            //             }
            //         }
            //     }
            // }
            
            for e in 0..w.grad.data.len() {
                w.grad.data[e] =  w.grad.data[e] / bsz; // get mean gradient of batch
                //-gradient--clipping-
                if w.grad.data[e] > 1.0 {
                    w.grad.data[e] = 1.0;
                }
                if w.grad.data[e] < -1.0 {
                    w.grad.data[e]=  -1.0;
                }
                //-------------------
                let m1 = prev_m1.data[e];
                let m2 = prev_m2.data[e];
                let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.data[e]);
                let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.data[e].powf(2.));
                w.tensor.data[e] =  w.tensor.data[e]-(alpha*biased_m1/(biased_m2.sqrt()+epsilon));
                prev_m1.data[e] =  biased_m1;
                prev_m2.data[e] = biased_m2;
            }
            k += 1
        }
        // println!("op {}", w.op);
        match &mut w.left {
            Some(x) => {
                k = self.step(x, bsz, k);
            }
            None => {}
        }
        match &mut w.right {
            Some(x) => {
                k = self.step(x, bsz, k);
            }
            None => {}
        }
        k
    }

    // Exponential learning rate decay
    fn exp_lr_decay(&mut self, initial_lr: f64, decay_rate: f64, decay_steps: f64) {
        self.alpha = initial_lr * (decay_rate.powf(self.t/decay_steps))
    }
}

// ----------------------------------- Training Utility Funcs -----------------------------------

fn init_best(best: &mut Vec<Tensor>, w: &Node) {
    if w.op == "param" {
        let p1 = Tensor::zeros(&w.tensor.shape);
        best.push(p1);
    }
    match &w.left {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
    match &w.right {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
}
fn save_best(best: &mut Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        best[k] = w.tensor.copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    k
}
fn load_best(best: & Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        w.tensor = best[k].copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    k
}

fn init_batch_grads(grads: &mut Vec<Tensor>, w: &Node) {
    if w.op == "param" {
        let p1 = Tensor::zeros(&w.grad.shape);
        grads.push(p1);
    }
    match &w.left {
        Some(x) => {
            init_batch_grads(grads, x);
        }
        None => {}
    }
    match &w.right {
        Some(x) => {
            init_batch_grads(grads, x);
        }
        None => {}
    }
}
fn add_batch_grads(grads: &mut Vec<Tensor>, w: &Node, mut k: usize) -> usize{
    if w.op == "param" {
        // _add_same_size(&mut grads[k], &w.grad);
        grads[k] = _add_same_size_fast(&grads[k], &w.grad);
        k += 1;
   }
   match &w.left {
       Some(x) => {
           k = add_batch_grads(grads, x, k);
       }
       None => {}
   }
   match &w.right {
       Some(x) => {
           k = add_batch_grads(grads, x, k);
       }
       None => {}
   }
   k
}
fn retrive_batch_grads(grads: & Vec<Tensor>, w: &mut Node, mut k: usize) -> usize{
    if w.op == "param" {
        w.grad = grads[k].copy_tens();
        k += 1;
    }
    match &mut w.left {
       Some(x) => {
           k = retrive_batch_grads(grads, x, k);
       }
       None => {}
    }
    match &mut w.right {
       Some(x) => {
           k = retrive_batch_grads(grads, x, k);
       }
       None => {}
    }
    k
}

// ASSUMING STEP STARTS AT INDEX 0
fn batch(step: usize, grads: &mut Vec<Tensor>, w: &mut Node, opt: &mut Adam, bsz: usize, training_set_size: usize, initial_lr: f64) {
    add_batch_grads(grads, w, 0);
    if (step+1) % bsz == 0 {
        retrive_batch_grads(grads, w, 0);
        opt.step(w, bsz as f64, 0);
        opt.exp_lr_decay(initial_lr, 0.95, (training_set_size/bsz) as f64);
        for t in 0..grads.len() {
            grads[t] = Tensor::zeros(&grads[t].shape)
        }
    }
}

// Simultaneously shuffle two vectors of tensors. (for shuffling x and y training set)
fn shuffle_xy(x: &mut Vec<Tensor>, y: &mut Vec<Tensor>) {
    let mut rng = rand::thread_rng();
    let len = x.len();
    for i in 0..len {
        let next = rng.gen_range(i..len);
        let tmpx = x[i].copy_tens();
        let tmpy = y[i].copy_tens();
        x[i] = x[next].copy_tens();
        y[i] = y[next].copy_tens();
        x[next] = tmpx;
        y[next] = tmpy;
    }
}
// ----------------------------------- FORWARD AND BACKWARD: -----------------------------------

// would probably be more efficient for one data tensor to get passed thru forward instead of each node having its own data tensor.

// These should probably be passing references to the root instead of the root itself.

// Forward Pass function. Recursively finds leaf nodes and gets their values, then performs calculations at each node until it gets back to the root (the output layer).
fn forward(mut root: Box<Node>, f_data: &Tensor) -> Box<Node> {
    // println!("op {} start", root.op);
    if root.op == "input" {
		root.tensor = f_data.copy_tens();
        return root;
	} else if root.op == "leaf" || root.op == "param" {
		return root;
	} else {
		let zeroed_data = Tensor::zeros(&root.tensor.shape);
		root.tensor = zeroed_data;
        // println!("op: {} | shape {:?}", root.op, root.tensor.shape);
	}
	if root.op == "+" { // add
		// Return tensor a + b
        let a1 = root.left.unwrap();
		let x1 = root.right.unwrap();
        let (left,right) = rayon::join(|| forward(a1, f_data), || forward(x1, f_data));
        root.tensor.shape[0] = right.tensor.shape[0];
        root.tensor = _add_bsz_times(&right.tensor, &left.tensor); // Assuming right tensor, aka x, is batched input
        root.left = Some(left);
        root.right = Some(right);
	} else if root.op == "mm" { // matmul
        let a1 = root.left.unwrap();
		let x1 = root.right.unwrap();
        let (a,x) = rayon::join(|| forward(a1, f_data), || forward(x1, f_data));
        root.tensor.shape[0] = x.tensor.shape[0];
        // println!("r: {:?}", root.tensor.shape);
        // println!("x: {:?}", x.tensor.shape);
        // println!("a: {:?}", a.tensor.shape);


        // let data = &mut root.tensor;
        // println!("a {:?}", a.tensor.shape);
        // println!("x {:?}", x.tensor.shape);
        // println!("root {:?}", data.shape);
		// for k in 0..x.tensor.l2() {
        //     for i in 0..a.tensor.l2() {
        //         for j in 0..a.tensor.l() {
        //             // println!("a: [{},{}], x: [{},{}], root: [{},{}]", i,j,k,j,k,i);
        //             let temp2 = a.tensor.get( &[i,j]) * x.tensor.get( &[k, j]);
        //             let temp = data.get( &[k, i]) + temp2;
        //             data.set( &[k, i], temp);
        //         }
        //     }
		// }
        
        // let mut v: Vec<f64> = Vec::new();
        // for k in 0..x.tensor.l2() {
        //     v.extend::<Vec<f64>>((0..a.tensor.l2()).into_par_iter().map( |i| {
        //             let mut temp = data.get(&[k, i]);
        //             for j in 0..a.tensor.l() {
        //                 temp += a.tensor.get(&[i,j]) * x.tensor.get( &[k, j]);
        //             };
        //             temp
        //         }
        //     ).collect()); 
        // }
        // root.tensor.data = v;

        let mut v: Vec<f64> = Vec::new();
        let jj = x.tensor.l(); 
        for i in 0..x.tensor.bsz() { // for each batch
            v.extend::<Vec<f64>>((0..a.tensor.l2()).into_par_iter().map( |k| { // for each node
                zip(
                    &x.tensor.data[x.tensor._get_ind(&[i,0,0])..x.tensor._get_ind(&[i,0,jj])], // get the vector of the current batch
                    &a.tensor.data[a.tensor._get_ind(&[0,k,0])..a.tensor._get_ind(&[0,k,jj])] // get the vector of the current node
                ).map(|(a,x)| {a * x}).sum()
            }).collect()); 
        }
        root.tensor.data = v;

        root.left = Some(a);
        root.right = Some(x);
       
	} else if root.op == "sm" { // Log Softmax
		let a = forward(root.left.unwrap(), f_data);
        // println!("a {} | {:?}", a.tensor.data.len(), a.tensor.shape);
        root.tensor.shape[0] = a.tensor.shape[0];
        root.tensor.data = vec![0.0; a.tensor.data.len()];
        // println!("r {} | {:?}", root.tensor.data.len(), root.tensor.shape);
		for b in 0..a.tensor.bsz() {
			let mut layer_sum: f64 = 0.0;
			let mut layer_max: f64 = 0.0000001;
			// layer_max is for normalization
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    if a.tensor.get( &[b,i,j]) > layer_max {
                        layer_max = a.tensor.get( &[b,i,j])
                    }
                }
            }
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    layer_sum += (a.tensor.get( &[b,i,j]) - layer_max).exp();
                }
            }
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    root.tensor.set( &[b,i,j], a.tensor.get( &[b,i,j]) - layer_sum.ln() - layer_max)
                }
            }
		}
        root.left = Some(a);
	} else if root.op == "sig" { // Sigmoid
		let a = forward(root.left.unwrap(), f_data);
        for b in 0..a.tensor.bsz() {
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    root.tensor.set( &[b,i,j], 1.0/(1.0 + (-1.0*a.tensor.get( &[b,i,j])).exp()));
                }
            }
        }
        root.left = Some(a);
	} else if root.op == "do" {
        let a = forward(root.left.unwrap(), f_data);
        let drop_node = root.right.unwrap();
        let mut rng = rand::thread_rng();
		// for i in 0..a.tensor.l2() {
		// 	for j in 0..a.tensor.l() {
        //         let flip = rng.gen_bool(drop_node.tensor.get(&[0]));
        //         if flip == true {
        //             root.tensor.set( &[i, j], 0.0);
        //         } else {
        //             root.tensor.set( &[i, j], a.tensor.get(&[i,j]));
        //         }
		// 	}
		// }
        for i in 0..a.tensor.data.len() {
            let flip = rng.gen_bool(drop_node.tensor.get(&[0]) as f64);
            if flip == true {
                root.tensor.data[i] = 0.0;
            } else {
                root.tensor.data[i] = a.tensor.data[i];
            }
        }
        root.left = Some(a);
        root.right = Some(drop_node);
    } else if root.op == "relu" {
        let a = forward(root.left.unwrap(), f_data);
        let mut relued: Vec<f64> = Vec::new();
        relued.extend(a.tensor.data.iter().map(|e| {
            if *e < 0.0 {
                0.0
            } else {
                *e
            }}));
        root.tensor.data = relued;
        root.left = Some(a);
    } else if root.op == "c2d" {
        let a1 = root.left.unwrap();
		let x1 = root.right.unwrap();
        let (a,x) = rayon::join(|| forward(a1, f_data), || forward(x1, f_data));
        let data = &mut root.tensor;
        // println!("a {:?}", a.tensor.shape);
        // println!("x {:?}", x.tensor.shape);
        // for m in 0..(x.tensor.l()) { // Filter layer
        //     for l in 0..data.shape[0] { // Output dim 0, top index of kernel on input
        //         for k in 0..data.shape[1] { // Output dim 1, left index of kernel on input
        //             for i in 0..x.tensor.shape[0] { // Kernel dim 0
        //                 for j in 0..x.tensor.shape[1] { // Kernel dim 1
        //                     // println!("a: [{},{}], x: [{},{},{}], root: [{},{},{}]", i+l,j+k, i,j, m,l, k, m);
        //                     let temp2 = a.tensor.get(&[i+l,j+k]) * x.tensor.get(&[i, j, m]);
        //                     let temp = data.get(&[l, k, m]) + temp2;
        //                     data.set(&[l, k, m], temp);
        //                 }
        //             }
        //         }
        //     }
        // }

        let mut v: Vec<f64> = Vec::new();
        let mm = x.tensor.shape[2];
        let jj = x.tensor.shape[1];
        let ii = x.tensor.shape[0];
        let xd1 = x.tensor.diminc[1]; 
        if a.tensor.shape.len() == 3 {
            for batch in 0..a.tensor.shape[0] {
                for l in 0..data.shape[0] { // Output dim 0, top index of kernel on input
                    for k in 0..data.shape[1] { // Output dim 1, left index of kernel on input
                        v.extend::<Vec<f64>>((0..mm).into_par_iter().map( |m| {
                            zip(
                                &a.tensor.data[a.tensor._get_ind(&[batch,l, k])..=a.tensor._get_ind(&[batch,l+ii-1, k+jj-1])], 
                                &mut x.tensor.data[m..].iter().step_by(xd1),
                            ).map(|(a,x)| {a * x}).sum()
                        }).collect()); 
                    }
                }
            }
        } else {
            for l in 0..data.shape[0] { // Output dim 0, top index of kernel on input
                for k in 0..data.shape[1] { // Output dim 1, left index of kernel on input
                    v.extend::<Vec<f64>>((0..mm).into_par_iter().map( |m| {
                        zip(
                            &a.tensor.data[a.tensor._get_ind(&[l, k])..=a.tensor._get_ind(&[l+ii-1, k+jj-1])], 
                            &mut x.tensor.data[m..].iter().step_by(xd1),
                        ).map(|(a,x)| {a * x}).sum()
                    }).collect()); 
                }
            }
        }
        root.tensor.data = v;

        root.left = Some(a);
        root.right = Some(x);

    } else if root.op == "reshape" {
        let a = forward(root.left.unwrap(), f_data);
        root.tensor = a.tensor.copy_tens();
        root.tensor.reshape(&root.grad.shape);
        root.left = Some(a);
    } else if root.op == "mp" {
        // let a = root.left.unwrap();
        // println!("mp before forward | a shape: {:?} | a datalen: {:?}", a.tensor.shape, a.tensor.data.len());
        // root.left = Some(a);
        let a = forward(root.left.unwrap(), f_data);

        // println!("start of mp | a shape: {:?} | a datalen: {:?}", a.tensor.shape, a.tensor.data.len());
        // println!("start of mp | r shape: {:?} | r datalen: {:?}", root.tensor.shape, root.tensor.data.len());

        let data = &mut root.tensor;
        // let l2dif = a.tensor.shape[0] - data.shape[0];
        // let ldif = a.tensor.l2() - data.l2();
        // for m in 0..a.tensor.l() { // filter layer, left intact
        //     for l in 0..data.shape[0] { // dimension 0 of output
        //         for k in 0..data.l2() { // dimension 1 of output
        //             let mut max = 0.0;
        //             for i in 0..(1+l2dif) { // dimension 0 of maxpool kernel
        //                 for j in 0..(1+ldif) { // dimension 1 of maxpool kernel
        //                     // println!("a: [{},{}], root: [{},{}]", i+ l,j+ k, l, k);
        //                     let temp = a.tensor.get(&[i + l, j+ k, m]);
        //                     if temp > max {
        //                         max = temp;
        //                     }
        //                 }
        //             }
        //             data.set(&[l, k, m], max);
        //         }
        //     }
        // }
        let l2dif = a.tensor.shape[0] / data.shape[0];
        let ldif = a.tensor.l2() / data.l2();
        for m in 0..a.tensor.l() { // filter layer, left intact
            for l in 0..data.shape[0] { // dimension 0 of output
                for k in 0..data.l2() { // dimension 1 of output
                    let mut max = f64::MIN;
                    for i in 0..(l2dif) { // dimension 0 of maxpool kernel
                        for j in 0..(ldif) { // dimension 1 of maxpool kernel
                            let temp = a.tensor._safe_get(&[i + l*l2dif, j+ k*ldif, m], "maxpool");
                            if temp > max {
                                max = temp;
                            }
                        }
                    }
                    data.set(&[l, k, m], max);
                }
            }
        }
        

        // println!("");
        // println!("a tens: {:?}", a.tensor.data);
        // println!("r tens: {:?}", root.tensor.data);
        // println!("");

        root.left = Some(a);

    } else if root.op == "sum_dim" {
        let a = forward(root.left.unwrap(), f_data);
        // let mm = a.tensor.shape[a.tensor.shape.len()-1];
        // let mut m = 0;
        // let mut summer = 0.0;
        // for i in 0..a.tensor.data.len() {
        //     summer += a.tensor.data[i];
        //     m += 1;
        //     if m+1 == mm {
        //         root.tensor.data[i/(mm-1)] = summer;
        //         m = 0;
        //         summer = 0.0;
        //     }
        // }
        for i in 0..a.tensor.shape[0] {
            for j in 0..a.tensor.shape[1] {
                let mut summer = 0.0;
                for k in 0..a.tensor.shape[2] {
                    summer += a.tensor.get(&[i,j,k]);
                }
                root.tensor.set(&[i,j], summer);
            }
        }
        root.left = Some(a);
    }
    // println!("op {} end", root.op);
    // println!("END op: {} | shape {:?}", root.op, root.tensor.shape);

    root
}

fn backward(mut root: Box<Node>) -> Box<Node>{
    // println!("back {}", root.op);
	if !root.require_grad {
		return root;
	} 
    // check options in left and right 
    match root.left {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.left = Some(x)
        }
    };
    match root.right {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.right = Some(x)
        }
    };
	if root.op == "+" { // add
		// Return tensor a + b
        let mut a = root.left.unwrap();
		let mut x = root.right.unwrap(); // Assuming right is batched input
        // let bias_data = Vec::new();
		// left.grad = root.grad.copy_tens();
        // right.grad = root.grad.copy_tens();
        // root.left = Some(backward(left));
        // root.right = Some(backward(right));
        let jj = a.grad.l(); 
        // let mut va: Vec<f64> = Vec::new();
        // va.extend::<Vec<f64>>((0..jj).into_par_iter().map( |j| { // for each coefficient
        //     &mut root.grad.data[j..].iter().step_by(jj).sum()
        // })); 
        let mut va: Vec<f64> = Vec::new();
        (0..jj).into_par_iter().map( |j| { // for each coefficient
            root.grad.data[j..].iter().step_by(jj).sum()
        }).collect_into_vec(&mut va); 
        a.grad.data = va;
        if x.require_grad {
           x.grad = root.grad.copy_tens();
        }
        let (a1,x1) = rayon::join(|| backward(a), || backward(x));
        root.left = Some(a1);
        root.right = Some(x1);
	} else if root.op == "mm" { // matmul
        let mut a = root.left.unwrap();
		let mut x = root.right.unwrap();

        // println!("r: {:?}", root.grad.shape);
        // println!("x: {:?}", x.grad.shape);
        // println!("a: {:?}", a.grad.shape);

		// for i in 0..a.tensor.l2() {
        //     for k in 0..x.tensor.l2() {
        //         for j in 0..a.tensor.l() {
        //             let rootgrad = root.grad.get( &[k, i]);
		// 			let t1 = a.grad.get( &[i, j]) + (x.tensor.get( &[k, j]) * rootgrad);
		// 			let t2 = x.grad.get( &[k, j]) + (a.tensor.get( &[i, j]) * rootgrad);
		// 			a.grad.set( &[i, j], t1);
		// 			x.grad.set( &[k, j], t2);
        //         }
        //     }
		// }

        // let mut va: Vec<f64> = Vec::new();
        // for i in 0..a.tensor.l2() {
        //     va.extend::<Vec<f64>>((0..a.tensor.l()).into_par_iter().map( |j| {
        //             let mut temp = a.grad.get(&[i, j]);
        //             for k in 0..x.tensor.l2() {
        //                 temp += x.tensor.get(&[k, j]) * root.grad.get(&[k, i]);
        //             };
        //             temp
        //         }
        //     ).collect()); 
        // }
        // a.grad.data = va;

        // let mut vx: Vec<f64> = Vec::new();
        // for k in 0..x.tensor.l2() {
        //     vx.extend::<Vec<f64>>((0..x.tensor.l()).into_par_iter().map( |j| {
        //             let mut temp = x.grad.get( &[k, j]);
        //             for i in 0..a.tensor.l2() {
        //                 temp += a.tensor.get( &[i, j]) * root.grad.get( &[k, i]);
        //             };
        //             temp
        //         }
        //     ).collect()); 
        // }
        // x.grad.data = vx;

        let jj = x.tensor.l(); 
        let mut va: Vec<f64> = Vec::new();
        for k in 0..a.tensor.l2() {
            va.extend::<Vec<f64>>((0..a.tensor.l()).into_par_iter().map( |j| { // for each coefficient
                zip(
                    &mut x.tensor.data[j..].iter().step_by(jj), // for each batch, get the element that is multiplied by the current coefficient j
                    &mut root.grad.data[k..].iter().step_by(x.tensor.bsz()) // for each batch, get the gradient of the current node k
                ).map(|(x,r)| {x * r}).sum()
            }).collect()); 
        }
        a.grad.data = va;
        if x.require_grad {
            let mut vx: Vec<f64> = Vec::new();
            for b in 0..x.tensor.bsz() {
                vx.extend::<Vec<f64>>((0..x.tensor.l()).into_par_iter().map( |j| { // for each element in input
                    zip(
                        &mut a.tensor.data[j..].iter().step_by(jj), //for each node, get the coefficient for the current element of a
                        & root.grad.data[
                            root.grad._get_ind(&[b, 0, 0]).. 
                            root.grad._get_ind(&[b, 0, a.tensor.l2()])
                        ] // also get the gradient of that node for the current batch
                    
                    ).map(|(a,r)| {a * r}).sum()
                }).collect()); 
            }
            x.grad.data = vx;
        }
        let (a1,x1) = rayon::join(|| backward(a), || backward(x));
        root.left = Some(a1);
        root.right = Some(x1);
	} else if root.op == "sm" { // Log Softmax
		let mut a = root.left.unwrap();
        for b in 0..root.tensor.bsz() {
            for i in 0..root.tensor.l2() {
                for j in 0..root.tensor.l() {
                    a.grad.set( &[b,i,j], -1. * (root.grad.get(&[b,i,j]) - root.tensor.get(&[b,i,j]).exp()))
                }
            }
        }
        root.left = Some(backward(a));
	} else if root.op == "sig" { // Sigmoid
		let mut a = root.left.unwrap();
        for b in 0..root.tensor.bsz() {
            for i in 0..root.tensor.l2() {
                for j in 0..root.tensor.l() {
                    a.grad.set( &[b,i,j], root.tensor.get( &[b,i,j]) * (1.0 - root.tensor.get( &[b,i,j])) * root.grad.get( &[b,i,j]));
                }
            }
        }
        root.left = Some(backward(a));
	} else if root.op == "do" || root.op == "relu" {
        let mut a = root.left.unwrap();
        for i in 0..a.tensor.data.len() {
            if a.tensor.data[i] > 0.0 {
                a.grad.data[i] = root.grad.data[i]
            }
        }
        root.left = Some(backward(a));
    } else if root.op == "c2d" {
        let mut a = root.left.unwrap();
        let mut x = root.right.unwrap();
        // println!("r: {:?}", root.grad.shape);
        // println!("x: {:?}", x.grad.shape);
        // println!("a: {:?}", a.grad.shape);
        // let data = &root.tensor;
        // for m in 0..(x.tensor.l()) { // Filter Layer
        //     for l in 0..data.shape[0] {
        //         for k in 0..data.shape[1] {
        //             for i in 0..x.tensor.shape[0] {
        //                 for j in 0..x.tensor.shape[1] {
        //                     let rootgrad = root.grad.get( &[l, k, m]);
        //                     let t1 = a.grad.get( &[i + l, j + k]) + (x.tensor.get(&[i, j, m]) * rootgrad);
        //                     let t2 = x.grad.get( &[i, j, m]) + (a.tensor.get(&[i + l, j + k]) * rootgrad);
        //                     a.grad.set( &[i + l, j + k], t1);
        //                     x.grad.set( &[i, j, m], t2);
        //                 }
        //             }
        //         }
        //     }
        // }

        let mut va: Vec<f64> = Vec::new();
        let mut vx: Vec<f64> = Vec::new();
        let mm = x.tensor.shape[2];
        let jj = x.tensor.shape[1];
        let ii = x.tensor.shape[0];
        let aj = a.tensor.l();
        let ai = a.tensor.l2();
        // let ll = root.tensor.shape[0];
        // let kk = root.tensor.shape[1];
        
        if a.require_grad {
            for a1 in 0..a.grad.shape[0] {
                va.extend::<Vec<f64>>((0..a.grad.shape[1]).into_par_iter().map( |a2| {
                    // let mut t3:usize = 0;
                    // let mut t4:usize = 0;
                    // let mut t1:usize = ii-1;
                    // let mut t2:usize = jj-1;
                    // let mut t5:usize = 0;
                    // let mut t6:usize = 0;
                    // let mut t7:usize = a1 + (ii-1);
                    // let mut t8:usize = a2 + (jj-1);
                    // if a1+1 >= ll{
                    //     t3 = 1+ a1 - ll;
                    // }
                    // if a2+1 >= kk {
                    //     t4 = 1+a2-kk;
                    // }
                    // if ii >= a1+1{
                    //     t1 = a1;
                    // }
                    // if jj >= a2+1 {
                    //     t2 = a2;
                    // }
                    // if a2 + 1 > jj {
                    //     t6 = a2 + 1 - jj;
                    // }
                    // if a1 + 1 > ii {
                    //     t5 = a1 + 1 - ii;
                    // }
                    // if  t7 >= ll{
                    //     t7 = ll - 1;
                    // }
                    // if t8 >= kk {
                    //     t8 = kk - 1;
                    // }

                    // zip(&mut x.tensor.data[
                    //     x.tensor._get_ind(&[t3,t4,0]).. // will almost always be zero since usize can only be positive
                    //     x.tensor._get_ind(&[t1,t2,mm])
                    // ].iter().step_by(x.tensor.diminc.iter().sum()),
                    // // ALERT ALERT ALERT
                    // &mut root.grad.data[
                    //     root.grad._get_ind(&[t5,t6,0]).. // will almost always be zero since usize can only be positive
                    //     root.grad._get_ind(&[t7,t8,mm])
                    // ].iter().step_by(mm)
                    // ).map(|(x,r)| {x * r}).sum()

                    // let mut p = false;
                    // if t7 - t5 != t1 - t3 {
                    //     p = true;
                    //     println!("conv i not working {} {}", t7 - t5, t1 - t3 );
                    // }
                    // if t8 - t6 != t2 - t4 {
                    //     p = true;
                    //     println!("conv j not working {} {}", t8 - t6,  t2 - t4);
                    // }
                    // if p {
                    //     panic!("");
                    // }
                    // let mut ret = 0.0;
                    // for qu in 0..(t1-t3) {
                    //     for qa in 0..(t2-t4) {
                    //         ret += x.tensor.get(&[t3 + qu, t4 + qa]) * root.grad.get(&[t5 + qu, t6 + qa]);
                    //     }
                    // }
                    // ret
                    let mut temp:f64 = 0.0;
                    for i in 0..ii {
                        for j in 0..jj {
                            if a1 >= i && a2 >= j && a1 + ii - (i+1) < ai && a2 + jj - (j+1) < aj {
                                // println!("{} {} | {} {}", x.tensor._get_ind(&[i,j,0]), x.tensor._get_ind(&[i,j,mm]), root.grad._get_ind(&[a1 - i,a2 - j,0]),root.grad._get_ind(&[a1 - i,a2 - j,mm]));
                                temp += zip(
                                    &mut x.tensor.data[
                                        x.tensor._get_ind(&[i,j,0]).. 
                                        x.tensor._get_ind(&[i,j,mm])
                                    ].iter(),
                                    &mut root.grad.data[
                                        root.grad._get_ind(&[a1 - i,a2 - j,0])..
                                        root.grad._get_ind(&[a1 - i,a2 - j,mm])
                                    ].iter()
                                ).map(|(x,r)| {x * r}).sum::<f64>();
                            }
                        }
                    }
                    temp
                }).collect());   
            }
        }
        for i in 0..x.tensor.shape[0] { // Kernel dim 0
            for j in 0..x.tensor.shape[1] { // Kernel dim 1
                vx.extend::<Vec<f64>>((0..mm).into_par_iter().map( |m| {
                    zip(
                        &mut a.tensor.data[
                            a.tensor._get_ind(&[i,j])..=
                            a.tensor._get_ind(&[ai-ii+i,aj-jj+j])
                            ].iter(), 
                        &mut root.grad.data[m..].iter().step_by(mm),
                    ).map(|(a,r)| {a * r}).sum()
                }).collect()); 
            }
        }

        if a.require_grad {
            a.grad.data = va;
        }
        x.grad.data = vx;
        
        let (a1,x1) = rayon::join(|| backward(a), || backward(x));
        root.left = Some(a1);
        root.right = Some(x1);
    } else if root.op == "reshape" {
        let mut a = root.left.unwrap();
        // root.grad.reshape(&a.grad.shape);
        a.grad = root.grad.copy_tens();
        // root.grad.reshape(&root.tensor.shape);

        a.grad.reshape(&a.tensor.shape);

        root.left = Some(backward(a));

    } else if root.op == "mp" {
        let mut a = root.left.unwrap();
        let data = &root.tensor;
        // let l2dif = a.tensor.shape[0] - data.shape[0];
        // let ldif = a.tensor.l2() - data.l2();
        // for m in 0..a.tensor.l() {
        //     for l in 0..data.shape[0] {
        //         for k in 0..data.l2() {
        //             let mut max = 0.0;
        //             let mut maxind = [0,0,0];
        //             for i in 0..(1 + l2dif) {
        //                 for j in 0..(1+ ldif) {
        //                     // println!("a: [{},{}], root: [{},{}]", i+ l,j+ k, l, k);
        //                     let temp = a.tensor.get(&[i + l, j+ k, m]);
        //                     if temp > max {
        //                         max = temp;
        //                         maxind = [i + l, j+ k, m];
        //                     }
        //                 }
        //             }
        //             a.grad.set(&maxind, root.grad.get(&[l,k]));
        //         }
        //     }
        // }
        let l2dif = a.tensor.shape[0] / data.shape[0];
        let ldif = a.tensor.l2() / data.l2();
        for m in 0..a.tensor.l() { // filter layer, left intact
            for l in 0..data.shape[0] { // dimension 0 of output
                for k in 0..data.l2() { // dimension 1 of output
                    let mut max = f64::MIN;
                    let mut maxind = [0,0,0];
                    for i in 0..(l2dif) { // dimension 0 of maxpool kernel
                        for j in 0..(ldif) { // dimension 1 of maxpool kernel
                            let temp = a.tensor.get(&[i + l*l2dif, j+ k*ldif, m]);
                            if temp > max {
                                max = temp;
                                maxind = [i + l*l2dif, j+ k*ldif, m];
                            }
                        }
                    }
                    a.grad.set(&maxind, root.grad.get(&[l,k,m]));
                }
            }
        }
        // println!("");
        // println!("a: {:?}", a.grad.data);
        // println!("r: {:?}", root.grad.data);
        // println!("");

        root.left = Some(backward(a));
    } else if root.op == "sum_dim" {
        let mut a = root.left.unwrap();
        // let mm = a.tensor.shape[a.tensor.shape.len()-1];
        // for g in 0..root.grad.data.len() {
        //     for m in 0..mm {
        //         a.grad.data[mm*g + m] = root.grad.data[g]
        //     }
        // }
        for i in 0..a.tensor.shape[0] {
            for j in 0..a.tensor.shape[1] {
                let g = root.grad.get(&[i,j]);
                for k in 0..a.tensor.shape[2] {
                    a.tensor.set(&[i,j,k], g);
                }
            }
        }
        root.left = Some(backward(a));
    }
    root
}

// ----------------------------------- TESTING (will be removed when done) -----------------------------------

fn _simple(shaper: &Vec<usize>) -> Box<Node> {
    let x_placeholder = Tensor::zeros(shaper);
	let mut x_node = leaf(x_placeholder, false);
    x_node.op = "input";

    let l1 = linear(x_node, 10);
    log_softmax(l1)

    // let l1 = linear(x_node, 100);
    // let s1 = sigmoid(l1);
    // let l2 = linear(s1, 10);
    // log_softmax(l2)

    // let r1 = reshape_node(x_node, vec![28,28]);
    // let c1 = conv2d(r1, vec![3,3], 64);
    // let re1 = relu(c1);
    // let m1 = max_pool(re1, vec![2,2]);
    // let c2 = conv2d(m1, vec![3,3], 32);
    // let re2 = relu(c2);
    // // let m2 = max_pool(re2, vec![2,2]);
    // let r2 = flatten(re2);
    // let l1 = linear(r2, 100);
    // let s1 = sigmoid(l1);
    // let l2 = linear(s1, 10);
    // log_softmax(l2)


    // let r = reshape_node(x_node, vec![28,28]);
    // let c = conv2d(r, vec![3,3], 32);
    // let r1 = relu(c);
    // let m = max_pool(r1, vec![2,2]);
    // // let s = _sum_dim(m);
    // let f = flatten(m);
    // let l1 = linear(f, 100);
    // let s1 = sigmoid(l1);
    // let l2 = linear(s1, 10);
    // log_softmax(l2)
}

pub(crate) fn simple() -> std::io::Result<()> {
    let dataset_size = 51200; // default: 51200
    let lr = 0.001; // default: 0.001
    let epochs = 10;

    // Two different implementations of minibatching.
    // If using one, the other should = 1.
    // batch_size is SAFER but SLOWER than bsz!!! (safer because 2d layers like conv2d and max pool are not set up to handle batched inputs)
    let _batch_size = 1; // each element in training set is a single input, batching is done via batch() function.
    let bsz = 64; // each element in training set is a batch of inputs.

// ========== read in data =============================
    let file = File::open("mnist_train.csv")?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut data: Vec<csv::StringRecord> = Vec::new();
    for result in rdr.records() {
        let record = result?;
        data.push(record);
    }

    let file2 = File::open("mnist_test.csv")?;
    let mut rdr2 = csv::Reader::from_reader(file2);
    let mut data2: Vec<csv::StringRecord> = Vec::new();
    for result in rdr2.records() {
        let record = result?;
        data2.push(record);
    }

    let mut x_train = Vec::new();
    let mut y_train = Vec::new();
    let mut last_ind = 0;
    for b in 0..(dataset_size / bsz) {
        // let data_slice = &data[b];
        let mut x = Tensor::zeros(&vec![bsz,1,784]);
        let mut y = Tensor::zeros(&vec![bsz,1,10]);
        for e in 0..x.bsz() {
            let data_slice = &data[b*x.bsz() + e];
            y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
            for i in 1..(x.l()+1) {
                x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
            }
        }
        x_train.push(x);
        y_train.push(y);
        last_ind += 1;
    }

    let mut x_test = Vec::new();
    let mut y_test = Vec::new();
    for b in 0..((x_train.len()/10+1)) { // /bsz
        // let data_slice = &data[b + last_ind];
        let mut x = Tensor::zeros(&vec![1,1,784]);
        let mut y = Tensor::zeros(&vec![1,1,10]);
        for e in 0..x.bsz() {
            let data_slice = &data[(last_ind+b)*x.bsz() + e];
            y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
            for i in 1..(x.l()+1) {
                x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
            }
        }
        x_test.push(x);
        y_test.push(y);
    }

    let mut x_val = Vec::new();
    let mut y_val = Vec::new();
    for b in 0..(10000) {
        // let data_slice = &data2[b];
        let mut x = Tensor::zeros(&vec![1,1,784]);
        let mut y = Tensor::zeros(&vec![1,1,10]);
        for e in 0..x.bsz() {
            let data_slice = &data2[b*x.bsz() + e];
            y.set( &[e,0,data_slice[0].parse::<usize>().unwrap()], 1.0);
            for i in 1..(x.l()+1) {
                x.set( &[e,0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
            }
        }
        x_val.push(x);
        y_val.push(y);
    }
// =========================================================


    let mut out = _simple(&x_train[0].shape);
    let mut opt = Adam {
        t: 0.0, 
        alpha: lr, 
        prev_m1s: Vec::new(),
        prev_m2s: Vec::new(),
    };
    opt.init_prevs(out.as_ref());
    let now_total = Instant::now();
    let mut best_epoch = 0;
    let mut best_epoch_loss = 999999.9;
    let mut best_params: Vec<Tensor> = Vec::new();
    init_best(best_params.as_mut(), out.as_ref());
    let mut batch_grads: Vec<Tensor> = Vec::new();
    init_batch_grads(&mut batch_grads, &out);
    println!("Starting at {:?}", Utc::now());
    println!("======= START TRAINING =======");
    for epoch in 0..epochs {
        let now = Instant::now();
        shuffle_xy(&mut x_train, &mut y_train);
        for i in 0..x_train.len() {
            let x = &x_train[i]; 
            let y = &y_train[i];
            // println!("staring forward ====");
            out = forward(out, x);
            // println!("staring loss ====");
            nll_loss(&mut out, y);
            // println!("staring backward ====");
            out = backward(out);
            // batch(i, &mut batch_grads, &mut out, &mut opt, batch_size, dataset_size, lr);
            // println!("staring step ====");
            opt.step(&mut out, bsz as f64, 0);
            opt.exp_lr_decay(lr, 0.95, x_train.len() as f64);
            // println!("staring test ====");


            // if i == x_train.len() - 1  && epoch > 50{  // && epoch == epochs - 1
            //     let mut pred = out.tensor.copy_tens();
            //     for p in 0..pred.l() {
            //         pred.data[p] = (((pred.data[p]).exp() * 100.0) as u128) as f64 / 100.0;
            //     }
            //     println!(" y    | {:?}", &y.data);
            //     println!(" pred | {:?}", pred.data);
            // }
        }
        let elapsed_time = now.elapsed().as_secs();
        let extra_seconds = elapsed_time % 60;
        let mins = elapsed_time / 60;
        let mut total_loss = 0.0;

        // for t in 0..x_train.len() {
        //     let x = &x_train[t]; 
        //     let y = &y_train[t]; 
        //     out = forward(out, &x);
        //     let loss = nll_loss(&mut out, y);
        //     total_loss += loss;
        // }
        // println!("Epoch {} | {} | {} minutes {} seconds", epoch, total_loss / (x_train.len() as f64), mins, extra_seconds);
        

        for t in 0..x_test.len() {
            let x = &x_test[t]; 
            let y = &y_test[t]; 
            out = forward(out, &x);
            let loss = nll_loss(&mut out, y);
            total_loss += loss;
        }
        println!("Epoch {} | {} | {} minutes {} seconds", epoch, total_loss / (x_test.len() as f64), mins, extra_seconds);
        
        if total_loss < best_epoch_loss {
            best_epoch_loss = total_loss;
            best_epoch = epoch;
            save_best(best_params.as_mut(), out.as_mut(), 0);
        }
    }
   
    let total_elapsed = now_total.elapsed().as_secs();
    let tot_extra_seconds = total_elapsed % 60;
    let tot_mins = total_elapsed / 60;
    println!("Total training time: {} minutes {} seconds", tot_mins, tot_extra_seconds);
    println!("Best Epoch: {} | {}", best_epoch, best_epoch_loss / (x_test.len() as f64));
    let now_val = Instant::now();
    println!("=====VALIDATING======");
    load_best(best_params.as_ref(), out.as_mut(), 0);
    let mut correct = 0;
    let mut incorrect = 0;
    let mut total_loss = 0.0;
    for t in 0..x_val.len() {
        let x = &x_val[t]; 
        let y = &y_val[t]; 
        out = forward(out, &x);
        for b in 0..out.tensor.bsz(){
            for q in 0..out.tensor.l2() {
                let mut max_pred = 0.0;
                let mut max_ind = 0;
                for p in 0..out.tensor.l() {
                    let temp = out.tensor.get( &[b,q,p]).exp();
                    if temp > max_pred {
                        max_pred = temp;
                        max_ind = p;
                    }
                }
                let mut y_ind = 0;
                for p in 0..y.l() {
                    if y.get( &[b,q,p]) > 0.0 {
                        y_ind = p;
                        // break;
                    }
                }
                if max_ind == y_ind {
                    correct += 1;
                } else {
                    incorrect += 1;
                }
            }
        }
        let loss = nll_loss(&mut out, y);
        total_loss += loss;
    }
    println!("Validation | {} | {:#?}", total_loss / (x_val.len() as f64), now_val.elapsed());
    println!("Total correct:   {}", correct);
    println!("Total incorrect: {}", incorrect);
    let acc = correct * 100 / (correct + incorrect);// as f64 / 100.0;
    println!("Accuracy: {}%", acc);

    Ok(())
}

pub(crate) fn _c2d_test() {
    let mut a = Tensor::he(&vec![3,3]);
    let b = Tensor::ones(&vec![3,3]);
    a.set(&[0,0], 2.);
    a.set(&[0,1], 1.);
    a.set(&[1,0], 1.);
    a.set(&[1,1], 1.);
    a.set(&[0,2], 0.);
    a.set(&[1,2], 1.);
    a.set(&[2,0], 0.);
    a.set(&[2,2], 1.);
    a.set(&[2,1], 3.);
    a.reshape(&vec![1,9]);
    println!("a: {:#?}", a.data);
    // let mut x = Tensor::xavier(& [2,2]);
    let in1 = leaf(a, false);
    let in2 = reshape_node(in1, vec![3,3]);
    let mut l1 = conv2d(in2, vec![2,2],1);
    let mut x = l1.right.unwrap();
    x.tensor.set(&[0,0], 0.);
    x.tensor.set(&[0,1], 1.);
    x.tensor.set(&[1,0], 2.);
    x.tensor.set(&[1,1], 3.);
    l1.right = Some(x);
    let l1 = forward(l1, &b);
    println!("l1: {:#?}", l1.tensor.data);
    let l1 = backward(l1);
    let x = l1.right.unwrap();
    println!("l1: {:#?}", x.grad.data);

}
