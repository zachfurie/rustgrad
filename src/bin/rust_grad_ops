
use std::{io, fs::File};

use rand::prelude::*;
use csv;

// ----------------------------------- Notes:  ----------------------------------------------

// Need to reset random seed!!!


//  ----------------------------------- Data Structures:  -----------------------------------

struct Node {
    left: Option<Box<Node>>,
	right: Option<Box<Node>>,
    // left: Option<&'a Node<'a>>,
	// right: Option<&'a Node<'a>>,
	op: &'static str,
	tensor: Tensor,
	grad: Tensor,
	require_grad: bool,
}

struct Tensor {
	data:  Vec<f64>, // Float64 all tensor data in a single slice 
	shape: Vec<usize>, // Int  [i] = length of dimension i
}

//  ----------------------------------- Auxiliary Functions:  -----------------------------------

impl Tensor {
    fn zeros(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        for i in shaper {
            data_len *= i
        }
        Tensor {
            data:  vec![0.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec() // [i] = length of dimension i
        }
    }

    fn ones(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        for i in shaper {
            data_len *= i
        }
        Tensor {
            data:  vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec() // [i] = length of dimension i
        }
    }

    fn xavier(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        for i in shaper {
            data_len *= i
        }
        let std = (shaper[shaper.len()-1] as f64).sqrt();
        let mut rng = rand::thread_rng();
        let mut t = Tensor {
            data:  vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec() // [i] = length of dimension i
        };
        for i in 0..data_len {
            let y: f64 = rng.gen(); // generates a float between 0 and 1
            if rng.gen_bool(0.5) {
                t.data[i] = y / std; // generates a float between 0 and 1
            } else {
                t.data[i] = -1. * y / std; // generates a float between 0 and 1
            }
        };
        t
    }

    fn get(&self, ind: Vec<usize>) -> f64 {
        let mut index = 0;
        for (i,x) in ind[..ind.len()-1].iter().enumerate() {
            index += self.shape[i+1] * x
        };
        index += ind[ind.len()-1];
        self.data[index]
    }
    
    fn set(&mut self, ind: Vec<usize>, elem: f64) {
        let mut index = 0;
        for (i,x) in ind[..ind.len()-1].iter().enumerate() {
            index += self.shape[i+1] * x
        };
        index += ind[ind.len()-1];
        self.data[index] = elem;
    }

    fn l2(&self) -> usize {
        self.shape[self.shape.len()-2]
    }

    fn l(&self) -> usize {
        self.shape[self.shape.len()-1]
    }

    fn copy_tens(&self) -> Tensor {
        let new_shape = self.shape.to_vec();
        let new_data = self.data.to_vec();
        Tensor {
            data:  new_data, // all tensor data in a single slice 
            shape: new_shape, // [i] = length of dimension i
        }
    }
}

fn add_same_size(dst: &mut Tensor, t: &Tensor ) {
    for (i,_) in t.data.iter().enumerate() {
        dst.data[i] += t.data[i]
    }
}

//  ----------------------------------- BASIC OPS:  -----------------------------------

// Inputs, weights, etc
fn leaf<'a>(tens: Tensor, require_grad: bool) -> Box<Node> {
	let zero_grad = Tensor::ones(&tens.shape);
	Box::new(Node {
        left: None,
        right: None, 
        op: "leaf", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad
    })
}

// Syntactic sugar for leaf nodes that are weights
fn weight<'a>(shaper: Vec<usize>) -> Box<Node> {
	let tens = Tensor::xavier(&shaper);
    let zero_grad = Tensor::ones(&shaper);
	Box::new(Node {
        left: None,
        right: None, 
        op: "leaf", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Add two identically shaped tensors
fn add<'a>(node1: Box<Node>, node2: Box<Node>) -> Box<Node> { //&'a 
	let zero_grad = Tensor::ones(&node1.tensor.shape);
	let data = Tensor::zeros(&node1.tensor.shape);
	Box::new(Node{
        left: Some(node1), 
        right: Some(node2), 
        op: "+", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Matrix Multiplication
fn matmul(a: Box<Node>, x: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&vec![x.tensor.l2(), a.tensor.l2()]);
	let data = Tensor::zeros(&vec![x.tensor.l2(), a.tensor.l2()]);
	Box::new(Node{
        left: Some(a), 
        right: Some(x), 
        op: "mm",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// fn matmul<'a>(a: &'a Node, x: &'a Node) -> Node<'a> {
// 	let zero_grad = Tensor::ones(&vec![x.tensor.l2(), a.tensor.l2()]);
// 	let data = Tensor::zeros(&vec![x.tensor.l2(), a.tensor.l2()]);
// 	Node{
//         left: Some(&a), 
//         right: Some(&x), 
//         op: "mm", 
//         tensor: data, 
//         grad: zero_grad, 
//         require_grad: true,
//     }
// }

//  ----------------------------------- ML OPS:  -----------------------------------

// Linear layer. Returns output, weight, bias so that weight and bias can be put in params[].
fn linear<'a>(inn: Box<Node>, out_dim: usize) -> Box<Node> { //  Node<'a>,  Node<'a>) {
	let l_weight = weight(vec![out_dim, inn.tensor.l()]);
	let bias = weight(vec![inn.tensor.l2(), out_dim]);
	let in_times_weight = matmul(l_weight,inn);
	let plus_bias = add(in_times_weight, bias);
	plus_bias
}

// Log Softmax layer
fn log_softmax<'a>(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sm", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}



//  ----------------------------------- LOSS AND OPTIM:  -----------------------------------

// Negative Log Likelihood. Takes prediction tensor and target tensor as inputs, returns loss, and updates gradients from input
fn nll_loss(pred: &mut Box<Node>, target: Box<Tensor>) -> f64 { // mut gradients: Box<Tensor>
	let mut loss = 0.0;
	for j in 0..target.l2() {
		for i in 0..target.l() {
			let t = target.get(vec![j, i]);
			loss -= t * pred.tensor.get(vec![j, i]);
			pred.grad.set(vec![j, i], t);
		}
	}
    loss
}

// initialize adam optimizer
struct Adam {
	// b1 float64, b2 float64
	t:        f64,// init to 0
	alpha:    f64,
	prev_m1s: Vec<Tensor>,
	prev_m2s: Vec<Tensor>,
}

impl Adam {
// Adam Optimizer Step function
    fn step(&mut self, weights:  &mut Vec<&mut Node>, bsz: f64) {
        // parameters: would normally get these from adam_init, but keeping them here as defaults for now.
        let b1: f64 = 0.9;
        let b2: f64 = 0.999;
        let epsilon: f64 = 10.0_f64.powf(-8.0);
        // ------------
        self.t += 1.0;
        let t = self.t;
        let alpha = self.alpha * (1. - b2.powf(t)).sqrt() / (1. - b1.powf(t));
        for k in 0..weights.len() {
            let w = &mut weights[k];
            if !w.require_grad {
                continue
            }
            let prev_m1 = &mut self.prev_m1s[k];
            let prev_m2 = &mut self.prev_m2s[k];
            for i in 0..w.grad.l2() {
                for j in 0..w.grad.l() {
                    w.grad.set(vec![i,j], w.grad.get(vec![i,j])/bsz); // get mean gradient of batch
                    //-gradient--clipping-
                    if w.grad.get(vec![i,j]) > 1.0 {
                        w.grad.set(vec![i,j], 1.0);
                    }
                    if w.grad.get(vec![i,j]) < -1.0 {
                        w.grad.set(vec![i,j], -1.0);
                    }
                    //-------------------
                    let m1 = prev_m1.get(vec![i,j]);
                    let m2 = prev_m2.get(vec![i,j]);
                    let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.get(vec![i,j]));
                    let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.get(vec![i,j]).powf(2.));
                    w.tensor.set(vec![i,j], w.tensor.get(vec![i,j])-(alpha*biased_m1/(biased_m2.sqrt()+epsilon)));
                    prev_m1.set(vec![i,j], biased_m1);
                    prev_m2.set(vec![i,j], biased_m2);
                }
            }
        }
    }

    // Exponential learning rate decay
    fn exp_lr_decay(&mut self, decay_rate: f64, global_step: f64, decay_steps: f64) {
        self.alpha = self.alpha * (decay_rate.powf(global_step/decay_steps))
    }
}



// ----------------------------------- FORWARD AND BACKWARD: -----------------------------------

// would probably be more efficient for one data tensor to get passed thru forward instead of each node having its own data tensor.

// Forward Pass function. Recursively finds leaf nodes and gets their values, then performs calculations at each node until it gets back to the root (the output layer).
fn forward(mut root: Box<Node>, f_data: &Tensor) -> Box<Node> {
    if root.op == "input" {
		root.tensor = f_data.copy_tens();
        return root;
	} else if root.op == "leaf" {
		return root;
	} else {
		let zeroed_data = Tensor::zeros(&root.tensor.shape);
		root.tensor = zeroed_data;
	}
	if root.op == "+" { // add
		// Return tensor a + b
        let left = forward(root.left.unwrap(), &f_data);
		let right = forward(root.right.unwrap(), &f_data);
		for i in 0..root.tensor.shape[root.tensor.shape.len()-2] {
			for j in 0..root.tensor.shape[root.tensor.shape.len()-1] {
				root.tensor.set(vec![i, j], left.tensor.get(vec![i, j]) + right.tensor.get(vec![i, j]));
			};
		};
        root.left = Some(left);
        root.right = Some(right);
	} else if root.op == "mm" { // matmul
	    // root.left = Some(forward(Option::unwrap(root.left)));
        // root.right = Some(forward(Option::unwrap(root.right)));
        let a = forward(root.left.unwrap(), f_data);
		let x = forward(root.right.unwrap(), f_data);
        let mut data = root.tensor;
		for k in 0..x.tensor.l2() {
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    let temp2 = a.tensor.get(vec![i,j]) * x.tensor.get(vec![k, j]);
                    let temp = data.get(vec![k, i]) + temp2;
                    data.set(vec![k, i], temp)
                }
            }
		}
        root.left = Some(a);
        root.right = Some(x);
        root.tensor = data;
	} else if root.op == "sm" { // Softmax
		let a = forward(root.left.unwrap(), f_data);
		for i in 0..a.tensor.l2() {
			let mut layer_sum = 0.;
			let mut layer_max = 0.000000001;
			// layer_max is for normalization
			for j in 0..a.tensor.l() {
				if a.tensor.get(vec![i, j]) > layer_max {
					layer_max = a.tensor.get(vec![i, j])
				}
			}
			for j in 0..a.tensor.l() {
				layer_sum += a.tensor.get(vec![i, j]).exp() - layer_max;
			}
			for j in 0..a.tensor.l() {
				root.tensor.set(vec![i, j], a.tensor.get(vec![i, j]) - layer_sum.ln() - layer_max)
			}
		}
        root.left = Some(a);
	} 
    root
}

fn backward(mut root: Box<Node>) -> Box<Node>{
	if !root.require_grad {
		return root;
	} 
    // check options in left and right 
    match root.left {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.left = Some(x)
        }
    };
    match root.right {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.right = Some(x)
        }
    };
	if root.op == "+" { // add
		// Return tensor a + b
        let mut left = root.left.unwrap();
		let mut right = root.right.unwrap();
		left.grad = root.grad.copy_tens();
        right.grad = root.grad.copy_tens();
        root.left = Some(backward(left));
        root.right = Some(backward(right));
	} else if root.op == "mm" { // matmul
	    // root.left = Some(forward(Option::unwrap(root.left)));
        // root.right = Some(forward(Option::unwrap(root.right)));
        let mut a = root.left.unwrap();
		let mut x = root.right.unwrap();
        // println!("a tens {:?} | a grad {:?}", a.tensor.shape, a.grad.shape);
        // println!("x tens {:?} | x grad {:?}", x.tensor.shape, x.grad.shape);
        let data = root.tensor;
		for i in 0..a.tensor.l2() {
            for i2 in 0..x.tensor.l2() {
                for j in 0..a.tensor.l() {
                    // println!("{} {} {}", i2, i, j);
                    // println!("a tens {:?} | a grad {:?}", a.tensor.shape, a.grad.shape);
                    // println!("x tens {:?} | x grad {:?}", x.tensor.shape, x.grad.shape);
                    let rootgrad = root.grad.get(vec![i2, i]);
					let t1 = a.grad.get(vec![i, j]) + (x.tensor.get(vec![i2, j]) * rootgrad);
					let t2 = x.grad.get(vec![i2, j]) + (a.tensor.get(vec![i, j]) * rootgrad);
					a.grad.set(vec![i, j], t1);
					x.grad.set(vec![i2, j], t2);
                }
            }
		}
        root.left = Some(backward(a));
        root.right = Some(backward(x));
        root.tensor = data;
	} else if root.op == "sm" { // Softmax
		let mut a = root.left.unwrap();
		for i in 0..root.tensor.l2() {
			for j in 0..root.tensor.l() {
				a.grad.set(vec![i, j], -1.*(root.grad.get(vec![i, j])-root.tensor.get(vec![i, j]).exp()))
			}
		}
        root.left = Some(backward(a));
	} 
    root
}




// ----------------------------------- TESTING (will be removed when done) -----------------------------------

fn _simple(dim0: usize, dim1: usize) -> Box<Node> {
    let x_placeholder = Tensor::zeros(&vec![dim0, dim1]);
	let mut x_node = leaf(x_placeholder, false);
    x_node.op = "input";
    let l1 = linear(x_node, 10);
    log_softmax(l1)
}

pub(crate) fn simple() -> std::io::Result<()> {
    let batch_size = 512; //51200
    let lr = 0.001;

    // read in data
    let file = File::open("mnist_train.csv")?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut data: Vec<csv::StringRecord> = Vec::new();
    for result in rdr.records() {
        let record = result?;
        data.push(record);
    }
    let mut x_train = Vec::new();
    let mut y_train = Vec::new();
    // let mut x_test = Vec::new();
    // let mut y_test = Vec::new();
    let d_i = 0;
    for b in 0..batch_size {
        let data_slice = &data[b];
        let mut x = Tensor::zeros(&vec![1,784]);
        let mut y = Tensor::zeros(&vec![1,10]);
        y.set(vec![0, data_slice[0].parse::<usize>().unwrap()], 1.0);
        for i in 1..x.data.len() {
            x.set(vec![0,i-1], data_slice[i].parse::<f64>().unwrap());
        }
        x_train.push(x);
        y_train.push(y);
    }

    let mut out = _simple(1, 784);
    let prev_m1 = vec![Tensor::zeros(&vec![1,10]), Tensor::zeros(&vec![10,784])];
    let prev_m2 = vec![Tensor::zeros(&vec![1,10]), Tensor::zeros(&vec![10,784])];
    let mut opt = Adam {
        t: 0.0, 
        alpha: lr, 
        prev_m1s: prev_m1,
        prev_m2s: prev_m2,
    };
    println!("======= START TRAINING ======");
    for epoch in 0..10 {
        let mut total_loss: f64 = 0.;
        for batch in 0..x_train.len() {
            let x = x_train[batch].copy_tens();
            let y = y_train[batch].copy_tens();
            // let zero_grad = Tensor::zeros(&out.grad.shape);
			// out.grad = zero_grad;
            // println!("forward");
            out = forward(out, &x);
            // println!("loss");
            let loss = nll_loss(&mut out, Box::new(y));
            total_loss += loss;
            // println!("backward");
            out = backward(out);
            // println!("params");
            let mut params: Vec<&mut Node> = Vec::new();
            let mut l1p = out.left.unwrap();
            let mut l1b = l1p.right.unwrap();
            let mut l1i = l1p.left.unwrap();
            let mut l1w = l1i.left.unwrap();
            params.push(l1b.as_mut());
            params.push(l1w.as_mut());
            // println!("step");
            opt.step(&mut params, 1.0);
            l1i.left = Some(l1w);
            l1p.left = Some(l1i);
            l1p.right = Some(l1b);
            out.left = Some(l1p);
            println!("batch {} | {}", batch, loss);



        }
        println!("Epoch {} | {}", epoch, total_loss);
    }

    Ok(())
}


pub(crate) fn foward_test() {
    let mut a = Tensor::xavier(&vec![2,2]);
    let mut x = Tensor::xavier(&vec![2,2]);

    let ind = vec![0,0];
    a.set(ind, 1.);
    let ind = vec![0,1];
    a.set(ind, 2.);
    let ind = vec![1,0];
    a.set(ind, 3.);
    let ind = vec![1,1];
    a.set(ind, 4.);

    let ind = vec![0,0];
    x.set(ind, 1.);
    let ind = vec![0,1];
    x.set(ind, 2.);
    let ind = vec![1,0];
    x.set(ind, 3.);
    let ind = vec![1,1];
    x.set(ind, 4.);

    println!("a: {:?}", x.data);
    println!("x: {:?}", x.data);
    let in1 = leaf(a, false);
    let in2 = leaf(x, false);
    let l1 = linear(in1, 10);
    // println!("l1: {:?}", forward(l1, x).tensor.data);

}


pub(crate) fn test() {
    let s: Vec<usize> = vec![2,2];
    let mut a = Tensor::xavier(&s);
    let mut b = a.copy_tens();
    let ind = vec![0,0];
    a.set(ind, 1.);
    let ind = vec![0,1];
    a.set(ind, 2.);
    let ind = vec![1,0];
    a.set(ind, 3.);
    let ind = vec![1,1];
    a.set(ind, 4.);

    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", a.get(ind1), a.get(ind2));
    println!("{} {}", a.get(ind3), a.get(ind4));
    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", b.get(ind1), b.get(ind2));
    println!("{} {}", b.get(ind3), b.get(ind4));

    add_same_size(&mut b, &a);

    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", a.get(ind1), a.get(ind2));
    println!("{} {}", a.get(ind3), a.get(ind4));
    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", b.get(ind1), b.get(ind2));
    println!("{} {}", b.get(ind3), b.get(ind4));

    
}
