
use std::{fs::File, time::Instant};
use rand::prelude::*;
use csv;

// ----------------------------------- Notes:  ----------------------------------------------

// Need to reset random seed
// implement parallelism
// Maybe use mutable arrays instead of vecs

//  ----------------------------------- Data Structures:  -----------------------------------

struct Node {
    left: Option<Box<Node>>,
	right: Option<Box<Node>>,
	op: &'static str,
	tensor: Tensor,
	grad: Tensor,
	require_grad: bool,
}

struct Tensor {
	data:  Vec<f64>, // Float64 all tensor data in a single slice 
	shape: Vec<usize>, // Int  [i] = length of dimension i
    diminc: Vec<usize>,
}

//  ----------------------------------- Auxiliary Functions:  -----------------------------------

impl Tensor {
    fn zeros(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs = vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:  vec![0.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        }
    }

    fn ones(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs = vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        Tensor {
            data:  vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs
        }
    }

    fn xavier(shaper: &Vec<usize>) -> Tensor {
        let mut data_len: usize = 1;
        let mut incs = vec![0; shaper.len()];
        for i in 0..shaper.len() {
            data_len *= shaper[i];
            if i == shaper.len()-1 {
                incs[i] = 1;
            } else {
                let mut temp = 1;
                for ii in i + 1..shaper.len() {
                    temp *= shaper[ii]
                }
                incs[i] = temp 
            }
        }
        let std = (shaper[shaper.len()-1] as f64).sqrt();
        let mut rng = rand::thread_rng();
        let mut t = Tensor {
            data:  vec![1.; data_len], // all tensor data in a single slice 
            shape: shaper.to_vec(), // [i] = length of dimension i
            diminc: incs,
        };
        for i in 0..data_len {
            let y: f64 = rng.gen(); // generates a float between 0 and 1
            if rng.gen_bool(0.5) {
                t.data[i] = y / std; // generates a float between 0 and 1
            } else {
                t.data[i] = -1. * y / std; // generates a float between 0 and 1
            }
        };
        t
    }

    fn get(&self, ind: Vec<usize>) -> f64 {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index]
    }
    
    fn set(&mut self, ind: Vec<usize>, elem: f64) {
        let mut index = 0;
        for (i,x) in ind.iter().enumerate() {
            index += self.diminc[i] * x;
        };
        self.data[index] = elem;
    }

    fn l2(&self) -> usize {
        self.shape[self.shape.len()-2]
    }

    fn l(&self) -> usize {
        self.shape[self.shape.len()-1]
    }

    fn copy_tens(&self) -> Tensor {
        let new_shape = self.shape.to_vec();
        let new_data = self.data.to_vec();
        let new_incs = self.diminc.to_vec();
        Tensor {
            data:  new_data, // all tensor data in a single slice 
            shape: new_shape, // [i] = length of dimension i
            diminc: new_incs,
        }
    }
}

fn _add_same_size(dst: &mut Tensor, t: &Tensor ) {
    for (i,_) in t.data.iter().enumerate() {
        dst.data[i] += t.data[i]
    }
}

//  ----------------------------------- BASIC OPS:  -----------------------------------

// Inputs, weights, etc
fn leaf<'a>(tens: Tensor, require_grad: bool) -> Box<Node> {
	let zero_grad = Tensor::ones(&tens.shape);
	Box::new(Node {
        left: None,
        right: None, 
        op: "leaf", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad
    })
}

// Syntactic sugar for leaf nodes that are weights
fn weight<'a>(shaper: Vec<usize>) -> Box<Node> {
	let tens = Tensor::xavier(&shaper);
    let zero_grad = Tensor::ones(&shaper);
	Box::new(Node {
        left: None,
        right: None, 
        op: "param", 
        tensor: tens, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Add two identically shaped tensors
fn add<'a>(node1: Box<Node>, node2: Box<Node>) -> Box<Node> { //&'a 
	let zero_grad = Tensor::ones(&node1.tensor.shape);
	let data = Tensor::zeros(&node1.tensor.shape);
	Box::new(Node{
        left: Some(node1), 
        right: Some(node2), 
        op: "+", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Matrix Multiplication
fn matmul(a: Box<Node>, x: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&vec![x.tensor.l2(), a.tensor.l2()]);
	let data = Tensor::zeros(&vec![x.tensor.l2(), a.tensor.l2()]);
	Box::new(Node{
        left: Some(a), 
        right: Some(x), 
        op: "mm",
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

//  ----------------------------------- ML OPS:  -----------------------------------

// Linear layer. Returns output, weight, bias so that weight and bias can be put in params[].
fn linear<'a>(inn: Box<Node>, out_dim: usize) -> Box<Node> { //  Node<'a>,  Node<'a>) {
	let l_weight = weight(vec![out_dim, inn.tensor.l()]);
	let bias = weight(vec![inn.tensor.l2(), out_dim]);
	let in_times_weight = matmul(l_weight,inn);
	let plus_bias = add(in_times_weight, bias);
	plus_bias
}

// Log Softmax layer
fn log_softmax(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sm", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

// Sigmoid layer
fn sigmoid(inn: Box<Node>) -> Box<Node> {
	let zero_grad = Tensor::ones(&inn.tensor.shape);
	let data = Tensor::zeros(&inn.tensor.shape);
	Box::new(Node {
        left: Some(inn), 
        right: None, 
        op: "sig", 
        tensor: data, 
        grad: zero_grad, 
        require_grad: true,
    })
}

//  ----------------------------------- LOSS AND OPTIM:  -----------------------------------

// Negative Log Likelihood. Takes prediction tensor and target tensor as inputs, returns loss, and updates gradients from input
fn nll_loss(pred: &mut Box<Node>, target: &Tensor) -> f64 { // mut gradients: Box<Tensor>
	let mut loss = 0.0;
	for j in 0..target.l2() {
		for i in 0..target.l() {
			let t = target.get(vec![j, i]);
			loss -= t * pred.tensor.get(vec![j, i]);
			pred.grad.set(vec![j, i], t);
		}
	}
    loss
}

// initialize adam optimizer
struct Adam {
	// b1 float64, b2 float64
	t:        f64,// init to 0
	alpha:    f64,
	prev_m1s: Vec<Tensor>,
	prev_m2s: Vec<Tensor>,
}

impl Adam {
// Adam Optimizer Step function
    fn init_prevs(&mut self, w: &Node) {
        if w.op == "param" {
            let p1 = Tensor::zeros(&w.grad.shape);
            let p2 = Tensor::zeros(&w.grad.shape);
            self.prev_m1s.push(p1);
            self.prev_m2s.push(p2);
        }
        match &w.left {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
        match &w.right {
            Some(x) => {
                self.init_prevs(x);
            }
            None => {}
        }
    }
    //create prev_m1s and prev_m2s with same recursive search pattern (DFS)
    fn step(&mut self, w: &mut Node, bsz: f64, mut k: usize) -> usize {
        // parameters: would normally get these from adam_init, but keeping them here as defaults for now.
        let b1: f64 = 0.9;
        let b2: f64 = 0.999;
        let epsilon: f64 = 10.0_f64.powf(-8.0);
        // ------------
        // self.t += 1.0;
        if w.op == "param" {
            let t = self.t;
            let alpha = self.alpha * (1. - b2.powf(t)).sqrt() / (1. - b1.powf(t));
            let prev_m1 = &mut self.prev_m1s[k];
            let prev_m2 = &mut self.prev_m2s[k];
            for i in 0..w.grad.l2() {
                for j in 0..w.grad.l() {
                    w.grad.set(vec![i,j], w.grad.get(vec![i,j])/bsz); // get mean gradient of batch
                    //-gradient--clipping-
                    if w.grad.get(vec![i,j]) > 1.0 {
                        w.grad.set(vec![i,j], 1.0);
                    }
                    if w.grad.get(vec![i,j]) < -1.0 {
                        w.grad.set(vec![i,j], -1.0);
                    }
                    //-------------------
                    let m1 = prev_m1.get(vec![i,j]);
                    let m2 = prev_m2.get(vec![i,j]);
                    let biased_m1 = (m1 * b1) + ((1. - b1) * w.grad.get(vec![i,j]));
                    let biased_m2 = (m2 * b2) + ((1. - b2) * w.grad.get(vec![i,j]).powf(2.));
                    w.tensor.set(vec![i,j], w.tensor.get(vec![i,j])-(alpha*biased_m1/(biased_m2.sqrt()+epsilon)));
                    prev_m1.set(vec![i,j], biased_m1);
                    prev_m2.set(vec![i,j], biased_m2);
                }
            }
            k += 1
        }
        match &mut w.left {
            Some(x) => {
                // let mut left = x;
                k = self.step(x, bsz, k);
                // w.left = Some(x);
            }
            None => {}
        }
        match &mut w.right {
            Some(x) => {
                k = self.step(x, bsz, k);
            }
            None => {}
        }
        k
    }

    // Exponential learning rate decay
    fn exp_lr_decay(&mut self, initial_lr: f64, decay_rate: f64, global_step: f64, decay_steps: f64) {
        self.alpha = initial_lr * (decay_rate.powf(global_step/decay_steps))
    }
}


fn init_best(best: &mut Vec<Tensor>, w: &Node) {
    if w.op == "param" {
        let p1 = Tensor::zeros(&w.tensor.shape);
        best.push(p1);
    }
    match &w.left {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
    match &w.right {
        Some(x) => {
            init_best(best, x);
        }
        None => {}
    }
}
fn save_best(best: &mut Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        best[k] = w.tensor.copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = save_best(best, x, k);
        }
        None => {}
    }
    k
}
fn load_best(best: & Vec<Tensor>, w: &mut Node, mut k: usize) -> usize {
    if w.op == "param" {
        w.tensor = best[k].copy_tens();
        k += 1;
    }
    match &mut w.left {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    match &mut w.right {
        Some(x) => {
            k = load_best(best, x, k);
        }
        None => {}
    }
    k
}

// ----------------------------------- FORWARD AND BACKWARD: -----------------------------------

// would probably be more efficient for one data tensor to get passed thru forward instead of each node having its own data tensor.

// Forward Pass function. Recursively finds leaf nodes and gets their values, then performs calculations at each node until it gets back to the root (the output layer).
fn forward(mut root: Box<Node>, f_data: &Tensor) -> Box<Node> {
    if root.op == "input" {
		root.tensor = f_data.copy_tens();
        return root;
	} else if root.op == "leaf" || root.op == "param" {
		return root;
	} else {
		let zeroed_data = Tensor::zeros(&root.tensor.shape);
		root.tensor = zeroed_data;
	}
	if root.op == "+" { // add
		// Return tensor a + b
        let left = forward(root.left.unwrap(), &f_data);
		let right = forward(root.right.unwrap(), &f_data);
		for i in 0..root.tensor.shape[root.tensor.shape.len()-2] {
			for j in 0..root.tensor.shape[root.tensor.shape.len()-1] {
				root.tensor.set(vec![i, j], left.tensor.get(vec![i, j]) + right.tensor.get(vec![i, j]));
			};
		};
        root.left = Some(left);
        root.right = Some(right);
	} else if root.op == "mm" { // matmul
        let a = forward(root.left.unwrap(), f_data);
		let x = forward(root.right.unwrap(), f_data);
        let mut data = root.tensor;
		for k in 0..x.tensor.l2() {
            for i in 0..a.tensor.l2() {
                for j in 0..a.tensor.l() {
                    let temp2 = a.tensor.get(vec![i,j]) * x.tensor.get(vec![k, j]);
                    let temp = data.get(vec![k, i]) + temp2;
                    data.set(vec![k, i], temp)
                }
            }
		}
        root.left = Some(a);
        root.right = Some(x);
        root.tensor = data;
	} else if root.op == "sm" { // Log Softmax
		let a = forward(root.left.unwrap(), f_data);
		for i in 0..a.tensor.l2() {
			let mut layer_sum: f64 = 0.0;
			let mut layer_max: f64 = 0.0000001;
			// layer_max is for normalization
			for j in 0..a.tensor.l() {
				if a.tensor.get(vec![i, j]) > layer_max {
					layer_max = a.tensor.get(vec![i, j])
				}
			}
			for j in 0..a.tensor.l() {
				layer_sum += (a.tensor.get(vec![i, j]) - layer_max).exp();
			}
			for j in 0..a.tensor.l() {
				root.tensor.set(vec![i, j], a.tensor.get(vec![i, j]) - layer_sum.ln() - layer_max)
			}
		}
        root.left = Some(a);
	} else if root.op == "sig" { // Sigmoid
		let a = forward(root.left.unwrap(), f_data);
		for i in 0..a.tensor.l2() {
			for j in 0..a.tensor.l() {
				root.tensor.set(vec![i, j], 1.0/(1.0 + (-1.0*a.tensor.get(vec![i, j])).exp()));
			}
		}
        root.left = Some(a);
	}
    root
}

fn backward(mut root: Box<Node>) -> Box<Node>{
	if !root.require_grad {
		return root;
	} 
    // check options in left and right 
    match root.left {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.left = Some(x)
        }
    };
    match root.right {
        None => {}
        Some(mut x) => {
            let zeroed_data = Tensor::zeros(&x.grad.shape);
            x.grad = zeroed_data;
            root.right = Some(x)
        }
    };
	if root.op == "+" { // add
		// Return tensor a + b
        let mut left = root.left.unwrap();
		let mut right = root.right.unwrap();
		left.grad = root.grad.copy_tens();
        right.grad = root.grad.copy_tens();
        root.left = Some(backward(left));
        root.right = Some(backward(right));
	} else if root.op == "mm" { // matmul
        let mut a = root.left.unwrap();
		let mut x = root.right.unwrap();
        let data = root.tensor;
		for i in 0..a.tensor.l2() {
            for i2 in 0..x.tensor.l2() {
                for j in 0..a.tensor.l() {
                    let rootgrad = root.grad.get(vec![i2, i]);
					let t1 = a.grad.get(vec![i, j]) + (x.tensor.get(vec![i2, j]) * rootgrad);
					let t2 = x.grad.get(vec![i2, j]) + (a.tensor.get(vec![i, j]) * rootgrad);
					a.grad.set(vec![i, j], t1);
					x.grad.set(vec![i2, j], t2);
                }
            }
		}
        root.left = Some(backward(a));
        root.right = Some(backward(x));
        root.tensor = data;
	} else if root.op == "sm" { // Log Softmax
		let mut a = root.left.unwrap();
		for i in 0..root.tensor.l2() {
			for j in 0..root.tensor.l() {
				a.grad.set(vec![i, j], -1.*(root.grad.get(vec![i, j])-root.tensor.get(vec![i, j]).exp()))
			}
		}
        root.left = Some(backward(a));
	} else if root.op == "sig" { // Sigmoid
		let mut a = root.left.unwrap();
		for i in 0..root.tensor.l2() {
			for j in 0..root.tensor.l() {
				a.grad.set(vec![i, j], root.tensor.get(vec![i, j]) * (1.0 - root.tensor.get(vec![i, j])) * root.grad.get(vec![i, j]));
			}
		}
        root.left = Some(backward(a));
	} 
    root
}




// ----------------------------------- TESTING (will be removed when done) -----------------------------------

fn _simple(dim0: usize, dim1: usize) -> Box<Node> {
    let x_placeholder = Tensor::zeros(&vec![dim0, dim1]);
	let mut x_node = leaf(x_placeholder, false);
    x_node.op = "input";
    let l1 = linear(x_node, 10);
    let s1 = sigmoid(l1);
    let l2 = linear(s1, 10);
    log_softmax(l2)
}

pub(crate) fn simple() -> std::io::Result<()> {
    let batch_size = 5120; //51200
    let lr = 0.001;
    let epochs = 10;

    // read in data
    let file = File::open("mnist_train.csv")?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut data: Vec<csv::StringRecord> = Vec::new();
    for result in rdr.records() {
        let record = result?;
        data.push(record);
    }

    let file2 = File::open("mnist_test.csv")?;
    let mut rdr2 = csv::Reader::from_reader(file2);
    let mut data2: Vec<csv::StringRecord> = Vec::new();
    for result in rdr2.records() {
        let record = result?;
        data2.push(record);
    }

    let mut x_train = Vec::new();
    let mut y_train = Vec::new();
    let mut last_ind = 0;
    for b in 0..batch_size {
        let data_slice = &data[b];
        let mut x = Tensor::zeros(&vec![1,784]);
        let mut y = Tensor::zeros(&vec![1,10]);
        y.set(vec![0, data_slice[0].parse::<usize>().unwrap()], 1.0);
        for i in 1..x.data.len() {
            x.set(vec![0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
        }
        x_train.push(x);
        y_train.push(y);
        last_ind += 1;
    }

    let mut x_test = Vec::new();
    let mut y_test = Vec::new();
    for b in 0..(batch_size/10+1) {
        let data_slice = &data[b + last_ind];
        let mut x = Tensor::zeros(&vec![1,784]);
        let mut y = Tensor::zeros(&vec![1,10]);
        y.set(vec![0, data_slice[0].parse::<usize>().unwrap()], 1.0);
        for i in 1..x.data.len() {
            x.set(vec![0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
        }
        x_test.push(x);
        y_test.push(y);
    }

    let mut x_val = Vec::new();
    let mut y_val = Vec::new();
    for b in 0..10000 {
        let data_slice = &data2[b];
        let mut x = Tensor::zeros(&vec![1,784]);
        let mut y = Tensor::zeros(&vec![1,10]);
        y.set(vec![0, data_slice[0].parse::<usize>().unwrap()], 1.0);
        for i in 1..x.data.len() {
            x.set(vec![0,i-1], data_slice[i].parse::<f64>().unwrap() / 255.0);
        }
        x_val.push(x);
        y_val.push(y);
    }

    let mut out = _simple(1, 784);
    let mut opt = Adam {
        t: 0.0, 
        alpha: lr, 
        prev_m1s: Vec::new(),
        prev_m2s: Vec::new(),
    };
    opt.init_prevs(out.as_ref());
    let now_total = Instant::now();
    let mut best_epoch = 0;
    let mut best_epoch_loss = 999999.9;
    let mut best_params: Vec<Tensor> = Vec::new();
    init_best(best_params.as_mut(), out.as_ref());
    println!("======= START TRAINING ======");
    for epoch in 0..epochs {
        let now = Instant::now();
        let mut for_tot = 0;
        let mut bac_tot = 0;
        let mut step_tot = 0;
        for batch in 0..x_train.len() {
            let x = &x_train[batch]; 
            let y = &y_train[batch];
            let for_time = Instant::now();
            out = forward(out, &x);
            for_tot += for_time.elapsed().as_millis();
            // if batch == x_train.len() - 1 && epoch == epochs - 1 { 
            //     let mut pred = out.tensor.copy_tens();
            //     for p in 0..pred.l() {
            //         pred.data[p] = (((pred.data[p]).exp() * 100.0) as u128) as f64 / 100.0;
            //     }
            //     println!(" y    | {:?}", &y.data);
            //     println!(" pred | {:?}", pred.data);
            // }

            nll_loss(&mut out, y);
            let bac_time = Instant::now();
            out = backward(out);
            bac_tot += bac_time.elapsed().as_millis();
            opt.t += 1.0;
            let step_time = Instant::now();
            opt.step(out.as_mut(), 1.0, 0);
            step_tot += step_time.elapsed().as_millis();
            // opt.exp_lr_decay(lr, 0.95, opt.t - 1.0, (epochs * x_train.len()) as f64);
        }
        let elapsed_time = now.elapsed().as_secs();
        let extra_seconds = elapsed_time % 60;
        let mins = elapsed_time / 60;
        let mut total_loss = 0.0;
        for t in 0..x_test.len() {
            let x = &x_test[t]; 
            let y = &y_test[t]; 
            out = forward(out, &x);
            let loss = nll_loss(&mut out, y);
            total_loss += loss;
        }
        println!("Epoch {} | {} | {} minutes {} seconds", epoch, total_loss / (x_test.len() as f64), mins, extra_seconds);
        if total_loss < best_epoch_loss {
            best_epoch_loss = total_loss;
            best_epoch = epoch;
            save_best(best_params.as_mut(), out.as_mut(), 0);
        }
        println!("forward time: {:#?} milliseconds", for_tot);
        println!("backward time: {:#?} milliseconds", bac_tot);
        println!("step time: {:#?} milliseconds", step_tot);
    }
   
    let total_elapsed = now_total.elapsed().as_secs();
    let tot_extra_seconds = total_elapsed % 60;
    let tot_mins = total_elapsed / 60;
    println!("Total training time: {} minutes {} seconds", tot_mins, tot_extra_seconds);
    println!("Best Epoch: {} | {}", best_epoch, best_epoch_loss / (batch_size as f64));
    let now_val = Instant::now();
    println!("=====VALIDATING======");
    load_best(best_params.as_ref(), out.as_mut(), 0);
    let mut correct = 0;
    let mut incorrect = 0;
    let mut total_loss = 0.0;
    for t in 0..x_val.len() {
        let x = &x_val[t]; //.copy_tens();
        let y = &y_val[t]; //.copy_tens();
        out = forward(out, &x);

        let mut max_pred = 0.0;
        let mut max_ind = 0;
        for p in 0..out.tensor.l() {
            let temp = out.tensor.get(vec![0, p]).exp();
            if temp > max_pred {
                max_pred = temp;
                max_ind = p;
            }
        }
        let mut y_ind = 0;
        for p in 0..y.l() {
            if y.get(vec![0,p]) > 0.0 {
                y_ind = p;
                // break;
            }
        }
        if max_ind == y_ind {
            correct += 1;
        } else {
            incorrect += 1;
        }

        let loss = nll_loss(&mut out, y);
        total_loss += loss;
    }
    println!("Validation | {} | {:#?}", total_loss / (x_val.len() as f64), now_val.elapsed());
    println!("Total correct:   {}", correct);
    println!("Total incorrect: {}", incorrect);
    let acc = (correct * 100 / (correct + incorrect)) as f64 / 100.0;
    println!("Accuracy: {}%", acc);

    Ok(())
}

pub(crate) fn get_test() {
    let mut a = Tensor::ones(&vec![10,10]);
    let mut t = Instant::now();
    for i in 0..10000000 {
        let b = a.get(vec![8,8]);
        a.set(vec![0,0], 10.0);
    }
    let tot = t.elapsed().as_millis();
    println!("{} ms", tot )
}

pub(crate) fn _foward_test() {
    let mut a = Tensor::xavier(&vec![2,2]);
    let mut x = Tensor::xavier(&vec![2,2]);

    let ind = vec![0,0];
    a.set(ind, 1.);
    let ind = vec![0,1];
    a.set(ind, 2.);
    let ind = vec![1,0];
    a.set(ind, 3.);
    let ind = vec![1,1];
    a.set(ind, 4.);

    let ind = vec![0,0];
    x.set(ind, 1.);
    let ind = vec![0,1];
    x.set(ind, 2.);
    let ind = vec![1,0];
    x.set(ind, 3.);
    let ind = vec![1,1];
    x.set(ind, 4.);

    println!("a: {:?}", x.data);
    println!("x: {:?}", x.data);
    let in1 = leaf(a, false);
    let _in2 = leaf(x, false);
    let _l1 = linear(in1, 10);
    // println!("l1: {:?}", forward(l1, x).tensor.data);

}


pub(crate) fn _test() {
    let s: Vec<usize> = vec![2,2];
    let mut a = Tensor::xavier(&s);
    let mut b = a.copy_tens();
    let ind = vec![0,0];
    a.set(ind, 1.);
    let ind = vec![0,1];
    a.set(ind, 2.);
    let ind = vec![1,0];
    a.set(ind, 3.);
    let ind = vec![1,1];
    a.set(ind, 4.);

    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", a.get(ind1), a.get(ind2));
    println!("{} {}", a.get(ind3), a.get(ind4));
    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", b.get(ind1), b.get(ind2));
    println!("{} {}", b.get(ind3), b.get(ind4));

    _add_same_size(&mut b, &a);

    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", a.get(ind1), a.get(ind2));
    println!("{} {}", a.get(ind3), a.get(ind4));
    let ind1 = vec![0,0];
    let ind2 = vec![0,1];
    let ind3 = vec![1,0];
    let ind4 = vec![1,1];
    println!("{} {}", b.get(ind1), b.get(ind2));
    println!("{} {}", b.get(ind3), b.get(ind4));

    
}
